{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a href='https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTableCreate/Drop/TruncateTable'>创建表</a>,<a href='https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-CreateTableCreate/Drop/TruncateTable'>加载数据</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建内部表表,创内部表,在删除表结构的时候,表数据也会被删除.\n",
    "    create table psn(\n",
    "            id int,\n",
    "            name string,\n",
    "            habit array<string>,\n",
    "            address map<string,string>\n",
    "        ) \n",
    "        ROW FORMAT DELIMITED \n",
    "            FIELDS TERMINATED BY ',' \n",
    "            COLLECTION ITEMS TERMINATED BY '-'\n",
    "            MAP KEYS TERMINATED BY ':'  \n",
    "            LINES TERMINATED BY '\\n';\n",
    "#### 加载文件    \n",
    "    LOAD DATA LOCAL INPATH '/root/data.txt'  INTO TABLE psn;\n",
    "#### 查看描述\n",
    "    desc formated psn;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建外部表,创建的时候要加入LOCATION,HIVE只维护表结构,不会动表数据\n",
    "     CREATE EXTERNAL TABLE page_view(\n",
    "         viewTime INT, \n",
    "         userid BIGINT,\n",
    "         page_url STRING, \n",
    "         referrer_url STRING,\n",
    "         ip STRING COMMENT 'IP Address of the User',\n",
    "         country STRING COMMENT 'country of origination'\n",
    "         )\n",
    "     COMMENT 'This is the staging page view table'\n",
    "         ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\054'\n",
    "         STORED AS TEXTFILE\n",
    "     LOCATION '<hdfs_location>';\n",
    "     \n",
    "     \n",
    "     CREATE EXTERNAL TABLE psn(\n",
    "         id INT, \n",
    "         name STRING,\n",
    "         habit ARRAY<STRING>,\n",
    "         address MAP<STRING,STRING>\n",
    "     )\n",
    "     ROW FORMAT DELIMITED \n",
    "         FIELDS TERMINATED BY ','\n",
    "         COLLECTION ITEMS TERMINATED BY '-'\n",
    "         MAP KEYS TERMINATED BY ':'  \n",
    "         LINES TERMINATED BY '\\n'\n",
    "     LOCATION '/usr';\n",
    "\n",
    "#### 加载文件    \n",
    "    LOAD DATA [LOCAL] INPATH '/data/mydata'  INTO TABLE psn;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.分区\n",
    "\n",
    "###                       a.创建分区\n",
    " #### 创建一个分区\n",
    "     create table psn(\n",
    "            id int,\n",
    "            name string,\n",
    "            habit array<string>,\n",
    "            address map<string,string>\n",
    "        ) \n",
    "        PARTITIONED BY (age int)\n",
    "        ROW FORMAT DELIMITED \n",
    "            FIELDS TERMINATED BY ',' \n",
    "            COLLECTION ITEMS TERMINATED BY '-'\n",
    "            MAP KEYS TERMINATED BY ':'  \n",
    "            LINES TERMINATED BY '\\n';\n",
    " #### 创建两个个分区\n",
    "      create table psn(\n",
    "            id int,\n",
    "            name string,\n",
    "            habit array<string>,\n",
    "            address map<string,string>\n",
    "        ) \n",
    "        PARTITIONED BY (age INT,sex STRING)\n",
    "        ROW FORMAT DELIMITED \n",
    "            FIELDS TERMINATED BY ',' \n",
    "            COLLECTION ITEMS TERMINATED BY '-'\n",
    "            MAP KEYS TERMINATED BY ':'  \n",
    "        LINES TERMINATED BY '\\n';\n",
    "#### 加载文件    \n",
    "    LOAD DATA [LOCAL] INPATH '/data/mydata'  INTO TABLE psn PARTITION (age=20, sex='M');\n",
    "#### 添加分区<a href='https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-AddPartitions'>help</a>:注意这个不是添加分区字段,是添加字段分区值\n",
    "    ALTER TABLE psn add PARTITION (age=10,sex='WM')\n",
    "#### 删除分区<a href='https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-DropPartitions'>help</a>\n",
    "    ALTER TABLE psn DROP PARTITION (sex='WM')\n",
    "#### 问题:外部表drop后会不会删除数据?\n",
    "    CREATE EXTERNAL TABLE psn(\n",
    "         id INT, \n",
    "         name STRING,\n",
    "         habit ARRAY<STRING>,\n",
    "         address MAP<STRING,STRING>\n",
    "     )\n",
    "     PARTITIONED BY (age int,sex string)\n",
    "     ROW FORMAT DELIMITED \n",
    "         FIELDS TERMINATED BY ','\n",
    "         COLLECTION ITEMS TERMINATED BY '-'\n",
    "         MAP KEYS TERMINATED BY ':'  \n",
    "         LINES TERMINATED BY '\\n';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.从其他表导入数据\n",
    "    FROM psn\n",
    "        INSERT OVERWRITE TABLE psn1  \n",
    "        SELECT id,name,habit\n",
    "        INSERT OVERWRITE TABLE psn2   \n",
    "        SELECT id,name\n",
    "\n",
    "\n",
    "    CREATE TABLE psn1(\n",
    "         id INT, \n",
    "         name STRING,\n",
    "         habit ARRAY<STRING>\n",
    "     )\n",
    "     ROW FORMAT DELIMITED \n",
    "         FIELDS TERMINATED BY ','\n",
    "         COLLECTION ITEMS TERMINATED BY '-' \n",
    "         LINES TERMINATED BY '\\n';\n",
    "         \n",
    "    CREATE TABLE psn2(\n",
    "         id INT, \n",
    "         name STRING\n",
    "     );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Hive Beeline(新的客户端) \n",
    "    需要配合server2服务使用,不用hive --service metastore &\n",
    "    1.在node2上面,启动服务 hiveserver2\n",
    "    2.beeline -u jdbc:hive2://node2:10000/default -n root\n",
    "    \n",
    "    命令以!开头\n",
    "    (2)、beeline\n",
    "    beeline> !connect jdbc:hive2://<host>:<port>/<db>;auth=noSasl root 123\n",
    "    \n",
    "    同时修改hadoop 配置文件 etc/hadoop/core-site.xml,加入如下配置项\n",
    "    <property>\n",
    "    <name>hadoop.proxyuser.root.hosts</name>\n",
    "    <value>*</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>hadoop.proxyuser.root.groups</name>\n",
    "        <value>*</value>\n",
    "    </property>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.作业\n",
    "   ### 1、实现struct例子:\n",
    "        1,xiaoming:22\n",
    "        2.xiaohong:33\n",
    "        \n",
    "        CREATE table student(\n",
    "            id string,\n",
    "            info struct<name:string,age:int>\n",
    "        )\n",
    "        ROW FORMAT DELIMITED \n",
    "            FIELDS TERMINATED BY ','\n",
    "            COLLECTION ITEMS TERMINATED BY ':';\n",
    "   ### 2、基站掉话率：找出掉线率最高的前10基站\n",
    "       原表:\n",
    "       create table call_monitor(\n",
    "            record_time string,\n",
    "            imei string,\n",
    "            cell string,\n",
    "            ph_num string,\n",
    "            call_num string,\n",
    "            drop_num double,\n",
    "            duration double,\n",
    "            drop_rate double,\n",
    "            net_type string,\n",
    "            erl string\n",
    "       )\n",
    "       ROW FORMAT DELIMITED \n",
    "            FIELDS TERMINATED BY ',';\n",
    "       加载数据\n",
    "       load data local inpath '/root/cdr_summ_imei_cell_info.csv' into table call_monitor; \n",
    "       \n",
    "       输出表:\n",
    "       create table call_result(\n",
    "            imei string,\n",
    "            drop_rate double\n",
    "       )\n",
    "       ROW FORMAT DELIMITED \n",
    "            FIELDS TERMINATED BY ',';\n",
    "       #插入输出\n",
    "       from call_monitor\n",
    "       insert into call_result\n",
    "           select imei,sum(drop_num)/sum(duration) drate group by imei order by drate desc\n",
    "   ### 3、使用hive实现wordcount\n",
    "       create table wordcount(\n",
    "           line ARRAY<STRING>\n",
    "       )\n",
    "       ROW FORMAT DELIMITED \n",
    "       COLLECTION ITEMS TERMINATED BY ' ';\n",
    "       \n",
    "       load data local inpath '/root/data/wc' into table wordcount;\n",
    "       \n",
    "       select word,count(word) from(select explode(line)  as word from wordcount) a group by a.word;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.参数调节\n",
    "    1.在hive命令行,s输入set; 查看所以参数设置\n",
    "    2.set hive.cli.print.header=true设置参数.\n",
    "    3.编辑home目录下的.hiverc文件,加入默认设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.动态分区\n",
    "   #### a)创建原始表:\n",
    "     create table people_orgin(\n",
    "            id int,\n",
    "            name string,\n",
    "            age int,\n",
    "            sex string,\n",
    "            habit array<string>,\n",
    "            address map<string,string>\n",
    "        ) \n",
    "        ROW FORMAT DELIMITED \n",
    "            FIELDS TERMINATED BY ',' \n",
    "            COLLECTION ITEMS TERMINATED BY '-'\n",
    "            MAP KEYS TERMINATED BY ':'  \n",
    "            LINES TERMINATED BY '\\n';\n",
    "       //加载数据\n",
    "       load data local inpath '/root/people' into table people_orgin;\n",
    "   #### b)创建分区表:\n",
    "        create table people_partition(\n",
    "            id int,\n",
    "            name string,\n",
    "            habit array<string>,\n",
    "            address map<string,string>\n",
    "        ) \n",
    "        PARTITIONED BY (age int,sex string)\n",
    "        ROW FORMAT DELIMITED \n",
    "            FIELDS TERMINATED BY ',' \n",
    "            COLLECTION ITEMS TERMINATED BY '-'\n",
    "            MAP KEYS TERMINATED BY ':'  \n",
    "            LINES TERMINATED BY '\\n';\n",
    "       //从原始表 加载数据\n",
    "       from people_orgin\n",
    "           insert overwrited people_partition select id,name,habit,address,age,sex distribute by age, sex;\n",
    "       //\n",
    "       from people_orgin\n",
    "           insert overwrite table people_partition select id,name,habit,address,age,sex distribute by age, sex;\n",
    "   ###  c)开启支持动态分区\n",
    "    set hive.exec.dynamic.partition=true;\n",
    "    默认：false\n",
    "        set hive.exec.dynamic.partition.mode=nostrict;\n",
    "    默认：strict（至少有一个分区列是静态分区）\n",
    "    相关参数\n",
    "        set hive.exec.max.dynamic.partitions.pernode;\n",
    "    每一个执行mr节点上，允许创建的动态分区的最大数量(1000)\n",
    "    set hive.exec.max.dynamic.partitions;\n",
    "    所有执行mr节点上，允许创建的所有动态分区的最大数量(1000)\n",
    "    set hive.exec.max.created.files;\n",
    "    所有的mr job允许创建的文件的最大数量(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.hive 分桶\n",
    "    分桶表是对列值取哈希值的方式，将不同数据放到不同文件中存储。\n",
    "    适用场景：\n",
    "        数据抽样（ sampling ）、map-join\n",
    "    1.开启分通:set hive.enforce.bucketing=true;//hive3.1不用设置?\n",
    "    2.创建分通表:\n",
    "     create table people_bucket(\n",
    "        id int,\n",
    "        name string,\n",
    "        age int,\n",
    "        sex string,\n",
    "        habit array<string>,\n",
    "        address map<string,string>\n",
    "    )\n",
    "    CLUSTERED BY (id) INTO 4 BUCKETS;\n",
    "    3.导入数据:\n",
    "        from people_orgin \n",
    "            insert overwrite table people_bucket select id,name,age,sex,habit,address; \n",
    "    4.抽样\n",
    "        select id, name, age from people_bucket tablesample(bucket 2 out of 4 on id);\n",
    "        从第二个bucket开始,每隔4个bucket,获得一个文件,//2这里有4个文件,所以取出2号文件,也就是id %4==1的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
