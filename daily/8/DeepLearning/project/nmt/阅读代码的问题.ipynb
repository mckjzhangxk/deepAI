{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchedInput(\n",
    "    collections.namedtuple(\"BatchedInput\",\n",
    "                           (\"initializer\", \"source\", \"target_input\",\n",
    "                            \"target_output\", \"source_sequence_length\",\n",
    "                            \"target_sequence_length\"))):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_iterator(src_dataset,\n",
    "                 tgt_dataset,\n",
    "                 src_vocab_table,\n",
    "                 tgt_vocab_table,\n",
    "                 batch_size,\n",
    "                 sos,\n",
    "                 eos,\n",
    "                 random_seed,\n",
    "                 num_buckets,\n",
    "                 src_max_len=None,\n",
    "                 tgt_max_len=None,\n",
    "                 num_parallel_calls=4,\n",
    "                 output_buffer_size=None,\n",
    "                 skip_count=None,\n",
    "                 num_shards=1,\n",
    "                 shard_index=0,\n",
    "                 reshuffle_each_iteration=True,\n",
    "                 use_char_encode=False):\n",
    "  output_buffer_size = batch_size * 1000\n",
    "\n",
    "\n",
    "  src_tgt_dataset = tf.data.Dataset.zip((src_dataset, tgt_dataset))\n",
    "\n",
    "  tgt_sos_id='0'\n",
    "  tgt_eos_id='1'\n",
    "  src_sos_id='0'\n",
    "  src_eos_id='1'\n",
    "\n",
    " #把一行转化成一个个单词序列\n",
    "  src_tgt_dataset = src_tgt_dataset.map(\n",
    "      lambda src, tgt: (\n",
    "          tf.string_split([src]).values, tf.string_split([tgt]).values),\n",
    "      num_parallel_calls=num_parallel_calls).prefetch(output_buffer_size)\n",
    "\n",
    "  # Filter zero length input sequences.\n",
    "  src_tgt_dataset = src_tgt_dataset.filter(\n",
    "      lambda src, tgt: tf.logical_and(tf.size(src) > 0, tf.size(tgt) > 0))\n",
    "\n",
    "\n",
    "  # Convert the word strings to ids.  Word strings that are not in the\n",
    "\n",
    "#   src_tgt_dataset = src_tgt_dataset.map(\n",
    "#         lambda src, tgt: (tf.cast(src_vocab_table.lookup(src), tf.int32),\n",
    "#                           tf.cast(tgt_vocab_table.lookup(tgt), tf.int32)),\n",
    "#         num_parallel_calls=num_parallel_calls)\n",
    "  #src_tgt_dataset是单词序列,序列值是单词索引\n",
    "  src_tgt_dataset = src_tgt_dataset.prefetch(output_buffer_size)\n",
    "  # Create a tgt_input prefixed with <sos> and a tgt_output suffixed with <eos>.\n",
    "  #对目标头为加入了tgt_sos_id,tgt_eos_id,(src,tgt)->(src,<sos>+tgt,tgt+<eos>)\n",
    "  src_tgt_dataset = src_tgt_dataset.map(\n",
    "      lambda src, tgt: (src,\n",
    "                        tf.concat(([tgt_sos_id], tgt), 0),\n",
    "                        tf.concat((tgt, [tgt_eos_id]), 0)),\n",
    "      num_parallel_calls=num_parallel_calls).prefetch(output_buffer_size)\n",
    "  # Add in sequence lengths.\n",
    "\n",
    "  src_tgt_dataset = src_tgt_dataset.map(\n",
    "        lambda src, tgt_in, tgt_out: (\n",
    "            src, tgt_in, tgt_out, tf.size(src), tf.size(tgt_in)),\n",
    "        num_parallel_calls=num_parallel_calls)\n",
    "  #src_tgt_dataset 表示(src,tgt_in,tgt_out,src.size,tgt_in.size)\n",
    "  src_tgt_dataset = src_tgt_dataset.prefetch(output_buffer_size)\n",
    "\n",
    "  # Bucket by source sequence length (buckets for lengths 0-9, 10-19, ...)\n",
    "  '''\n",
    "  x是一个(src,tgtin,tgtout,size(src),size(tgt))的数据集,\n",
    "  返回批处理的数据库,shape=(N,) 每个元素是一个(src,tgtin,tgtout,size(src),size(tgt))\n",
    "  由于每一行长度不同(src,tgin,tgout),会做pad处理,pad_value分别是src_eos_id,tgt_eos_id,tgt_eos_id\n",
    "  '''\n",
    "  def batching_func(x):\n",
    "    return x.padded_batch(\n",
    "        batch_size,\n",
    "        # The first three entries are the source and target line rows;\n",
    "        # these have unknown-length vectors.  The last two entries are\n",
    "        # the source and target row sizes; these are scalars.\n",
    "        padded_shapes=(\n",
    "            tf.TensorShape([None]),  # src\n",
    "            tf.TensorShape([None]),  # tgt_input\n",
    "            tf.TensorShape([None]),  # tgt_output\n",
    "            tf.TensorShape([]),  # src_len\n",
    "            tf.TensorShape([])),  # tgt_len\n",
    "        # Pad the source and target sequences with eos tokens.\n",
    "        # (Though notice we don't generally need to do this since\n",
    "        # later on we will be masking out calculations past the true sequence.\n",
    "        padding_values=(\n",
    "            src_eos_id,  # src\n",
    "            tgt_eos_id,  # tgt_input\n",
    "            tgt_eos_id,  # tgt_output\n",
    "            0,  # src_len -- unused\n",
    "            0))  # tgt_len -- unused\n",
    "\n",
    "\n",
    "  batched_dataset = batching_func(src_tgt_dataset)\n",
    "  batched_iter = batched_dataset.make_initializable_iterator()\n",
    "  (src_ids, tgt_input_ids, tgt_output_ids, src_seq_len,\n",
    "   tgt_seq_len) = (batched_iter.get_next())\n",
    "  return BatchedInput(\n",
    "      initializer=batched_iter.initializer,\n",
    "      source=src_ids,\n",
    "      target_input=tgt_input_ids,\n",
    "      target_output=tgt_output_ids,\n",
    "      source_sequence_length=src_seq_len,\n",
    "      target_sequence_length=tgt_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_dataset=tf.data.TextLineDataset('/home/zhangxk/AIProject/nmt/nmt/scripts/iwsl515/train.en')\n",
    "tgt_dataset=tf.data.TextLineDataset('/home/zhangxk/AIProject/nmt/nmt/scripts/iwsl515/train.vi')\n",
    "src_vocab_table=tf.contrib.lookup.index_table_from_file('/home/zhangxk/AIProject/nmt/nmt/scripts/iwsl515/vocab.en')\n",
    "tgt_vocab_table=tf.contrib.lookup.index_table_from_file('/home/zhangxk/AIProject/nmt/nmt/scripts/iwsl515/vocab.vi')\n",
    "batch_size=3\n",
    "sos,eos='<sos>','<eos>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batchinput=get_iterator(src_dataset,\n",
    "                 tgt_dataset,\n",
    "                 src_vocab_table,\n",
    "                 tgt_vocab_table,\n",
    "                 batch_size,\n",
    "                 sos,\n",
    "                 eos,\n",
    "                 random_seed=0,\n",
    "                 num_buckets=1,\n",
    "                 num_shards=1,\n",
    "                 shard_index=0,\n",
    "                 reshuffle_each_iteration=False,\n",
    "                 use_char_encode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(batchinput.source)\n",
    "print(batchinput.target_input)\n",
    "print(batchinput.source_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    sess.run(batchinput.initializer)\n",
    "    for i in range(2):\n",
    "        x=sess.run([batchinput.source,\n",
    "                     batchinput.target_input,\n",
    "                     batchinput.target_output,\n",
    "                    batchinput.source_sequence_length,\n",
    "                    batchinput.target_sequence_length])\n",
    "        print(x[0].shape)\n",
    "        print(x[1].shape)\n",
    "        print(x[2].shape)\n",
    "        print(x[3])\n",
    "        print(x[4])\n",
    "        print('xxxxxxxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x[0].shape)\n",
    "print(x[1].shape)\n",
    "print(x[2].shape)\n",
    "print(x[3])\n",
    "print(x[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-05b089ab135a>:6: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "BATCH,T,D=22,10,64\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[BATCH,T,D])\n",
    "with tf.variable_scope('my',tf.AUTO_REUSE):\n",
    "    ndims=[128,256,512]\n",
    "    cells=[tf.contrib.rnn.BasicLSTMCell(d) for d in ndims]\n",
    "#state_is_tuple,有多少成返回的state是个tuple,就有多少个元素,每个元素又是LSTMStateTuple,保存lstm的s,h,都是tensor类型\n",
    "    complete_cell=tf.contrib.rnn.MultiRNNCell(cells,state_is_tuple=True)\n",
    "    zero_state=complete_cell.zero_state(BATCH,dtype=tf.float32)\n",
    "    \n",
    "    states=zero_state\n",
    "    outputs=[]\n",
    "    for t in range(T):\n",
    "        output,states=complete_cell(X[:,t,:],states)\n",
    "        outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'my/my/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_2:0' shape=(22, 512) dtype=float32>, <tf.Tensor 'my/my/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_5:0' shape=(22, 512) dtype=float32>, <tf.Tensor 'my/my/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_8:0' shape=(22, 512) dtype=float32>, <tf.Tensor 'my/my/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_11:0' shape=(22, 512) dtype=float32>, <tf.Tensor 'my/my/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_14:0' shape=(22, 512) dtype=float32>, <tf.Tensor 'my/my/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_17:0' shape=(22, 512) dtype=float32>, <tf.Tensor 'my/my/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_20:0' shape=(22, 512) dtype=float32>, <tf.Tensor 'my/my/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_23:0' shape=(22, 512) dtype=float32>, <tf.Tensor 'my/my/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_26:0' shape=(22, 512) dtype=float32>, <tf.Tensor 'my/my/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_29:0' shape=(22, 512) dtype=float32>]\n",
      "10\n",
      "(LSTMStateTuple(c=<tf.Tensor 'my/my/multi_rnn_cell/cell_0/basic_lstm_cell/Add_19:0' shape=(22, 128) dtype=float32>, h=<tf.Tensor 'my/my/multi_rnn_cell/cell_0/basic_lstm_cell/Mul_29:0' shape=(22, 128) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'my/my/multi_rnn_cell/cell_1/basic_lstm_cell/Add_19:0' shape=(22, 256) dtype=float32>, h=<tf.Tensor 'my/my/multi_rnn_cell/cell_1/basic_lstm_cell/Mul_29:0' shape=(22, 256) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'my/my/multi_rnn_cell/cell_2/basic_lstm_cell/Add_19:0' shape=(22, 512) dtype=float32>, h=<tf.Tensor 'my/my/multi_rnn_cell/cell_2/basic_lstm_cell/Mul_29:0' shape=(22, 512) dtype=float32>))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#总结:对于MultiRNNCell单元,\\n\\n输入:\\n    X:[N,T]\\n    state:(l1_state,l2_state,...ln_state)\\n    li_state:LSTMStateTuple(c:Tensor,h:Tensor)\\n输出\\n    output:(N,ln_ndims),最后一次的h\\n    state:(l1_state,l2_state,...ln_state),li_state:LSTMStateTuple(c:Tensor,h:Tensor)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(outputs) #output是最高层的输出,shape(N,D_last_layer)\n",
    "print(len(outputs))\n",
    "print(states) #(outputs,(layer1_state,layer2_state,layer3_state))\n",
    "'''\n",
    "#总结:对于MultiRNNCell单元,\n",
    "\n",
    "输入:\n",
    "    X:[N,T]\n",
    "    state:(l1_state,l2_state,...ln_state)\n",
    "    li_state:LSTMStateTuple(c:Tensor,h:Tensor)\n",
    "输出\n",
    "    output:(N,ln_ndims),最后一次的h\n",
    "    state:(l1_state,l2_state,...ln_state),li_state:LSTMStateTuple(c:Tensor,h:Tensor)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "BATCH,T,D=22,10,64\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[BATCH,T,D])\n",
    "with tf.variable_scope('mMmM',reuse=tf.AUTO_REUSE):\n",
    "    ndims=[32,64]\n",
    "    cells=[tf.contrib.rnn.BasicLSTMCell(d) for d in ndims]\n",
    "    complete_cell=tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    \n",
    "    output,states=tf.nn.dynamic_rnn(complete_cell,X,\n",
    "                      sequence_length=None,\n",
    "                      initial_state=None,\n",
    "                      dtype=tf.float32,\n",
    "                      swap_memory=True,\n",
    "                      time_major=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mMmM/rnn/transpose_1:0\", shape=(22, 10, 64), dtype=float32)\n",
      "(LSTMStateTuple(c=<tf.Tensor 'mMmM/rnn/while/Exit_3:0' shape=(22, 32) dtype=float32>, h=<tf.Tensor 'mMmM/rnn/while/Exit_4:0' shape=(22, 32) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'mMmM/rnn/while/Exit_5:0' shape=(22, 64) dtype=float32>, h=<tf.Tensor 'mMmM/rnn/while/Exit_6:0' shape=(22, 64) dtype=float32>))\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#bi_direction rnn\n",
    "tf.reset_default_graph()\n",
    "BATCH,T,D=22,10,64\n",
    "X=tf.placeholder(dtype=tf.float32,shape=[BATCH,T,D])\n",
    "\n",
    "with tf.variable_scope('bi',reuse=tf.AUTO_REUSE):\n",
    "    ndims=[32,64]\n",
    "    cells=[tf.contrib.rnn.BasicLSTMCell(d) for d in ndims]\n",
    "    forward_cell=tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    #注意,要new一边不能用上面的cells,不然不会创建新的weight\n",
    "    cells=[tf.contrib.rnn.BasicLSTMCell(d) for d in ndims]\n",
    "    backward_cell=tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    \n",
    "    output,states=tf.nn.bidirectional_dynamic_rnn(forward_cell,backward_cell,X,dtype=tf.float32,time_major=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((LSTMStateTuple(c=<tf.Tensor 'bi/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(22, 32) dtype=float32>, h=<tf.Tensor 'bi/bidirectional_rnn/fw/fw/while/Exit_4:0' shape=(22, 32) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'bi/bidirectional_rnn/fw/fw/while/Exit_5:0' shape=(22, 64) dtype=float32>, h=<tf.Tensor 'bi/bidirectional_rnn/fw/fw/while/Exit_6:0' shape=(22, 64) dtype=float32>)), (LSTMStateTuple(c=<tf.Tensor 'bi/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(22, 32) dtype=float32>, h=<tf.Tensor 'bi/bidirectional_rnn/bw/bw/while/Exit_4:0' shape=(22, 32) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'bi/bidirectional_rnn/bw/bw/while/Exit_5:0' shape=(22, 64) dtype=float32>, h=<tf.Tensor 'bi/bidirectional_rnn/bw/bw/while/Exit_6:0' shape=(22, 64) dtype=float32>)))\n",
      "((LSTMStateTuple(c=<tf.Tensor 'tile_batch/Reshape:0' shape=(66, 32) dtype=float32>, h=<tf.Tensor 'tile_batch/Reshape_1:0' shape=(66, 32) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'tile_batch/Reshape_2:0' shape=(66, 64) dtype=float32>, h=<tf.Tensor 'tile_batch/Reshape_3:0' shape=(66, 64) dtype=float32>)), (LSTMStateTuple(c=<tf.Tensor 'tile_batch/Reshape_4:0' shape=(66, 32) dtype=float32>, h=<tf.Tensor 'tile_batch/Reshape_5:0' shape=(66, 32) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'tile_batch/Reshape_6:0' shape=(66, 64) dtype=float32>, h=<tf.Tensor 'tile_batch/Reshape_7:0' shape=(66, 64) dtype=float32>)))\n"
     ]
    }
   ],
   "source": [
    "# print(output)\n",
    "print(states)\n",
    "beam_states=tf.contrib.seq2seq.tile_batch(states,3)\n",
    "print(beam_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'bi/bidirectional_rnn/fw/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(96, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'bi/bidirectional_rnn/fw/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'bi/bidirectional_rnn/fw/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(96, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'bi/bidirectional_rnn/fw/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(256,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(20, 5, 2), dtype=int32)\n",
      "(<tf.Tensor 'tile_batch/Reshape:0' shape=(60, 5, 2) dtype=int32>, <tf.Tensor 'tile_batch/Reshape_1:0' shape=(60, 5, 2) dtype=int32>)\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "tf.reset_default_graph()\n",
    "s=tf.constant(value=1,shape=[20,5,2])\n",
    "h=tf.constant(value=1,shape=[20,5,2])\n",
    "print(s)\n",
    "c=tf.contrib.seq2seq.tile_batch((s,h),3)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
