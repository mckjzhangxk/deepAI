{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First review the base knowage of linear Algebra\n",
    "\n",
    "<a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.eig.html\">eig</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.74406752  3.57594683  3.01381688  2.72441591  2.118274  ]\n",
      " [ 3.22947057  2.18793606  4.458865    4.8183138   1.91720759]\n",
      " [ 3.95862519  2.6444746   2.84022281  4.62798319  0.35518029]\n",
      " [ 0.4356465   0.10109199  4.16309923  3.89078375  4.35006074]\n",
      " [ 4.89309171  3.99579282  2.30739681  3.90264588  0.59137213]]\n",
      "------------------\n",
      "[[ 2.74406752  3.57594683  3.01381688  2.72441591  2.118274  ]\n",
      " [ 3.22947057  2.18793606  4.458865    4.8183138   1.91720759]\n",
      " [ 3.95862519  2.6444746   2.84022281  4.62798319  0.35518029]\n",
      " [ 0.4356465   0.10109199  4.16309923  3.89078375  4.35006074]\n",
      " [ 4.89309171  3.99579282  2.30739681  3.90264588  0.59137213]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "A=np.random.rand(5,5)*5;\n",
    "print(A)\n",
    "print(\"------------------\")\n",
    "W,S=np.linalg.eig(A)\n",
    "S_inv=np.linalg.inv(S)\n",
    "B=S.dot(np.diag(W)).dot(S_inv)\n",
    "print(B.real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next we implement PR algrithm\n",
    "formula: pr_new=(1-d)/n+d* (W*pr_old) <br />\n",
    "d is prob that a user keep stay at remain sites(prob a given vertex jump to another vertex)<br />\n",
    "W[i][j]=I{j-->i}/degree(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PR(W,prob=0.85,t=10):\n",
    "    '''\n",
    "        W:relation matrix,wij=I[j->i] / degree(j)\n",
    "        prob:prob jump from one to another vertex\n",
    "        t:number of iteration\n",
    "        return:rank,rank score,decent order\n",
    "    '''\n",
    "    \n",
    "    n=W.shape[0]\n",
    "    #initial uniformly\n",
    "    init_score=np.ones(n)/n\n",
    "    \n",
    "    E,S=np.linalg.eig(W)\n",
    "    S_inv=np.linalg.inv(S)\n",
    "    \n",
    "    #resolve or project initial score to eig space S\n",
    "    resolve=S_inv.dot(init_score)\n",
    "    \n",
    "    score=np.real(S.dot(np.diag(E**t)).dot(resolve))\n",
    "    score=prob*score+(1-prob)/n\n",
    "    \n",
    "    \n",
    "    rank=np.argsort(-score)\n",
    "    rankScore=score[rank]\n",
    "    return rank,rankScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example from this video <a href='https://www.youtube.com/watch?v=P8Kt6Abq_rM'>video</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 3, 1, 0]), array([ 0.375 ,  0.3125,  0.1875,  0.125 ]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W=np.array([\n",
    "        [0,0,1/3,0],\n",
    "        [1/2,0,1/3,0],\n",
    "        [1/2,0,0,1],\n",
    "        [0,1,1/3,0]\n",
    "    ])\n",
    "PR(W,prob=1,t=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Now review some NLTK \n",
    "## TOKENIZE\n",
    "## Corpara: body of text\n",
    "## Lexicon:the mean of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize,word_tokenize\n",
    "#unfortunally this fail to tokenize chinere,using jieba instead\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msg=\"黄晓明工作室对此发声明否认，称黄晓明不认识高某，未参与过任何操纵股票的行为；黄晓明未曾受过任何与股票有关的处罚，亦未介入过任何与股票有关的调查。黄晓明工作室还敦促相关方即刻删撤一切虚假信息，以免对公众造成误导并导致己方法律责任的承担。返回搜狐\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.463 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.463 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['黄晓明',\n",
       " '工作室',\n",
       " '对此',\n",
       " '发声明',\n",
       " '否认',\n",
       " '，',\n",
       " '称',\n",
       " '黄晓明',\n",
       " '不',\n",
       " '认识',\n",
       " '高某',\n",
       " '，',\n",
       " '未',\n",
       " '参与',\n",
       " '过',\n",
       " '任何',\n",
       " '操纵',\n",
       " '股票',\n",
       " '的',\n",
       " '行为',\n",
       " '；',\n",
       " '黄晓明',\n",
       " '未曾',\n",
       " '受过',\n",
       " '任何',\n",
       " '与',\n",
       " '股票',\n",
       " '有关',\n",
       " '的',\n",
       " '处罚',\n",
       " '，',\n",
       " '亦',\n",
       " '未',\n",
       " '介入',\n",
       " '过',\n",
       " '任何',\n",
       " '与',\n",
       " '股票',\n",
       " '有关',\n",
       " '的',\n",
       " '调查',\n",
       " '。',\n",
       " '黄晓明',\n",
       " '工作室',\n",
       " '还',\n",
       " '敦促',\n",
       " '相关',\n",
       " '方',\n",
       " '即刻',\n",
       " '删撤',\n",
       " '一切',\n",
       " '虚假',\n",
       " '信息',\n",
       " '，',\n",
       " '以免',\n",
       " '对',\n",
       " '公众',\n",
       " '造成',\n",
       " '误导',\n",
       " '并',\n",
       " '导致',\n",
       " '己方',\n",
       " '法律责任',\n",
       " '的',\n",
       " '承担',\n",
       " '。',\n",
       " '返回',\n",
       " '搜狐']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(jieba.cut(msg,cut_all=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 黄晓明,pos:nr\n",
      "word 工作室,pos:n\n",
      "word 对此,pos:d\n",
      "word 发声明,pos:l\n",
      "word 否认,pos:v\n",
      "word ，,pos:x\n",
      "word 称,pos:v\n",
      "word 黄晓明,pos:nr\n",
      "word 不,pos:d\n",
      "word 认识,pos:v\n",
      "word 高某,pos:nr\n",
      "word ，,pos:x\n",
      "word 未,pos:d\n",
      "word 参与,pos:v\n",
      "word 过,pos:ug\n",
      "word 任何,pos:r\n",
      "word 操纵,pos:v\n",
      "word 股票,pos:n\n",
      "word 的,pos:uj\n",
      "word 行为,pos:v\n",
      "word ；,pos:x\n",
      "word 黄晓明,pos:nr\n",
      "word 未曾,pos:d\n",
      "word 受过,pos:v\n",
      "word 任何,pos:r\n",
      "word 与,pos:p\n",
      "word 股票,pos:n\n",
      "word 有关,pos:vn\n",
      "word 的,pos:uj\n",
      "word 处罚,pos:v\n",
      "word ，,pos:x\n",
      "word 亦,pos:d\n",
      "word 未,pos:d\n",
      "word 介入,pos:v\n",
      "word 过,pos:ug\n",
      "word 任何,pos:r\n",
      "word 与,pos:p\n",
      "word 股票,pos:n\n",
      "word 有关,pos:vn\n",
      "word 的,pos:uj\n",
      "word 调查,pos:vn\n",
      "word 。,pos:x\n",
      "word 黄晓明,pos:nr\n",
      "word 工作室,pos:n\n",
      "word 还,pos:d\n",
      "word 敦促,pos:v\n",
      "word 相关,pos:v\n",
      "word 方,pos:n\n",
      "word 即刻,pos:d\n",
      "word 删撤,pos:v\n",
      "word 一切,pos:r\n",
      "word 虚假,pos:a\n",
      "word 信息,pos:n\n",
      "word ，,pos:x\n",
      "word 以免,pos:c\n",
      "word 对,pos:p\n",
      "word 公众,pos:n\n",
      "word 造成,pos:v\n",
      "word 误导,pos:n\n",
      "word 并,pos:c\n",
      "word 导致,pos:v\n",
      "word 己方,pos:n\n",
      "word 法律责任,pos:n\n",
      "word 的,pos:uj\n",
      "word 承担,pos:v\n",
      "word 。,pos:x\n",
      "word 返回,pos:v\n",
      "word 搜狐,pos:nz\n"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "for char,pos in pseg.cut(msg):\n",
    "#     if pos.startswith('n') or pos.startswith('a'):\n",
    "    print('word %s,pos:%s'%(char,pos))\n",
    "#         words.append(char)\n",
    "words=set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some document about jieba <a href='https://github.com/fxsjy/jieba'>doc</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corpora2Sentence(corpos):\n",
    "    '''\n",
    "        cut corpos into sentences\n",
    "        return:\n",
    "            sentences:list,one element represent a none adj,select[i]=(word,pos) mean word as pos\n",
    "            vocabs:a set of all words occur in corpos\n",
    "            word_idx and idx_word:the map between idx and vocab,I using a matrix representation\n",
    "                for the graph\n",
    "            vocabs_position:a dict,the element look like: \n",
    "                V(i):[p1,p2,..pn],mean word V occur at positon p1,p2,..pn\n",
    "            position_vocabs:reverse of vocabs_position:\n",
    "                pi:V(j),mean at positon pi is word V(j)\n",
    "    '''\n",
    "    sentences=[]\n",
    "    vocabs=[]  #store all vocabulary\n",
    "    vocabs_position={}\n",
    "    word_idx=0 #the word position at corpos\n",
    "    for char,pos in pseg.cut(corpos):\n",
    "        word_idx+=1\n",
    "        if pos=='x':continue\n",
    "        elif (pos.startswith('n') or pos=='a')and len(char)>1:\n",
    "            sentences.append((char,word_idx))\n",
    "            vocabs.append(char)\n",
    "            if char in vocabs_position:vlist=vocabs_position[char]\n",
    "            else:\n",
    "                vlist=[]\n",
    "                vocabs_position[char]=vlist\n",
    "            vlist.append(word_idx)\n",
    "        \n",
    "        \n",
    "    vocabs=set(vocabs)\n",
    "    word_idx={c:i for i,c in enumerate(vocabs)}\n",
    "    idx_word={i:c for i,c in enumerate(vocabs)}\n",
    "    \n",
    "    position_vocabs={}\n",
    "    for vocb,poslist in vocabs_position.items():\n",
    "        for p in poslist:position_vocabs[p]=vocb\n",
    "\n",
    "    return sentences,vocabs,word_idx,idx_word,vocabs_position,position_vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('黄晓明', 1),\n",
       "  ('工作室', 2),\n",
       "  ('黄晓明', 8),\n",
       "  ('高某', 11),\n",
       "  ('股票', 18),\n",
       "  ('黄晓明', 22),\n",
       "  ('股票', 27),\n",
       "  ('股票', 38),\n",
       "  ('黄晓明', 43),\n",
       "  ('工作室', 44),\n",
       "  ('虚假', 52),\n",
       "  ('信息', 53),\n",
       "  ('公众', 57),\n",
       "  ('误导', 59),\n",
       "  ('己方', 62),\n",
       "  ('法律责任', 63),\n",
       "  ('搜狐', 68)],\n",
       " {'信息', '公众', '工作室', '己方', '搜狐', '法律责任', '股票', '虚假', '误导', '高某', '黄晓明'},\n",
       " {'信息': 0,\n",
       "  '公众': 2,\n",
       "  '工作室': 1,\n",
       "  '己方': 8,\n",
       "  '搜狐': 7,\n",
       "  '法律责任': 6,\n",
       "  '股票': 9,\n",
       "  '虚假': 5,\n",
       "  '误导': 10,\n",
       "  '高某': 3,\n",
       "  '黄晓明': 4},\n",
       " {0: '信息',\n",
       "  1: '工作室',\n",
       "  2: '公众',\n",
       "  3: '高某',\n",
       "  4: '黄晓明',\n",
       "  5: '虚假',\n",
       "  6: '法律责任',\n",
       "  7: '搜狐',\n",
       "  8: '己方',\n",
       "  9: '股票',\n",
       "  10: '误导'},\n",
       " {'信息': [53],\n",
       "  '公众': [57],\n",
       "  '工作室': [2, 44],\n",
       "  '己方': [62],\n",
       "  '搜狐': [68],\n",
       "  '法律责任': [63],\n",
       "  '股票': [18, 27, 38],\n",
       "  '虚假': [52],\n",
       "  '误导': [59],\n",
       "  '高某': [11],\n",
       "  '黄晓明': [1, 8, 22, 43]},\n",
       " {1: '黄晓明',\n",
       "  2: '工作室',\n",
       "  8: '黄晓明',\n",
       "  11: '高某',\n",
       "  18: '股票',\n",
       "  22: '黄晓明',\n",
       "  27: '股票',\n",
       "  38: '股票',\n",
       "  43: '黄晓明',\n",
       "  44: '工作室',\n",
       "  52: '虚假',\n",
       "  53: '信息',\n",
       "  57: '公众',\n",
       "  59: '误导',\n",
       "  62: '己方',\n",
       "  63: '法律责任',\n",
       "  68: '搜狐'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora2Sentence(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    def __init__(self,filename):\n",
    "        '''\n",
    "            windowSize:2 word's distance <=windowSize,will be view as a valid edge\n",
    "        '''\n",
    "        with open(filename) as f:\n",
    "            corpora=f.read()\n",
    "            self.sentence,self.vocab,self.word_idx,self.idx_word,self.vocabs_position,self.position_vocabs=corpora2Sentence(corpora)\n",
    "        self.words=len(self.vocab)\n",
    "        self.H=np.zeros((self.words,self.words))\n",
    "        \n",
    "    def _connect(self,char1,char2):\n",
    "        c1,p1,c2,p2=*char1,*char2\n",
    "        \n",
    "        if(abs(p2-p1)>self.windowSize):return False\n",
    "        idx1,idx2=self.word_idx[c1],self.word_idx[c2]\n",
    "        if idx1!=idx2:self.H[idx1,idx2]=self.H[idx2,idx1]=1\n",
    "        return True\n",
    "    def getMatrix(self,windowSize=5,eps=1e-6):\n",
    "        self.windowSize=windowSize\n",
    "        \n",
    "        sentencelen=len(self.sentence)\n",
    "        for i in range(sentencelen-1):\n",
    "            char1=self.sentence[i]\n",
    "            for j in range(i+1,sentencelen):\n",
    "                char2=self.sentence[j]\n",
    "                if self._connect(char1,char2)==False:break\n",
    "        degree=np.sum(self.H,axis=0)\n",
    "        degree[degree==0]=eps\n",
    "        H=self.H/degree\n",
    "        return H\n",
    "    def rank(self,indices,top=1000):\n",
    "        '''\n",
    "            indices:the word's index according to Page Rank Algrithm,in decent order\n",
    "            top:select first top word of indices\n",
    "        '''\n",
    "\n",
    "        \n",
    "        selectedWords=[]\n",
    "        for i in range(min(len(indices),top)):\n",
    "            selectedWords.append(self.idx_word[indices[i]])\n",
    "        \n",
    "        #all positon of selected word occur in corpos\n",
    "        sel_pos_list=[]\n",
    "        for w in selectedWords:\n",
    "            sel_pos_list.extend(self.vocabs_position[w])\n",
    "        sel_pos_list=sorted(sel_pos_list)\n",
    "        \n",
    "        #merged conceitive word\n",
    "        hasSelected=set()\n",
    "        i=0\n",
    "        while i<len(sel_pos_list):\n",
    "            pi=sel_pos_list[i]\n",
    "            Wi=self.position_vocabs[pi]\n",
    "            merge=[Wi]\n",
    "            for j in range(i+1,len(sel_pos_list)):\n",
    "                pj=sel_pos_list[j]\n",
    "                if (pj-pi)==(j-i):\n",
    "                    Wj=self.position_vocabs[pj]\n",
    "                    if Wj not in merge:merge.append(Wj)\n",
    "                else:\n",
    "                    break\n",
    "            merge='-'.join(merge)\n",
    "            hasSelected.add(merge)\n",
    "            if i==j:break;\n",
    "            i=j\n",
    "        return sorted(hasSelected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph=Graph('news.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'信息', '字段', '车型', '法律法规', '装备', '标记', '财政', '处分', '目录', '企业', '公告', '信息化', '办税', '技术标准', '船用', '性能指标', '双燃料', '发动机', '报告', '两用', '电式', '产品质量', '工业', '节约能源', '汽油', '专项', '司法机关', '车船', '利用', '规定', '车船税', '燃用', '动力电池', '进口量', '新疆生产建设兵团', '年度', '税务局', '现场', '计划单列市', '申报材料', '乘用车', '热值', '财政部', '委员会', '财政厅', '网站', '异议', '纳税人', '中国', '自治区', '视同', '委托', '电动', '样本', '具体', '技术', '程式', '责任', '管理工作', '附件', '公务员法', '排量', '虚假', '主管部门', '汽车', '核定', '直辖市', '国务院', '产业', '机构', '许可', '工况', '检验证书', '一致性', '管理系统', '新能源', '气动力', '所有人', '申报', '之日起', '中华人民共和国', '过程', '优惠政策', '中心', '真实性', '柴油', '证书', '方式', '行政监察', '比例', '资料', '船舶', '部门', '全部', '微量', '因素', '国家', '经销商', '产量', '符合标准', '方面', '办公厅', '财政局', '天然气', '资格', '商用车', '消耗量', '工作人员', '国家税务总局', '标准', '燃油', '结果', '涉嫌犯罪', '动力装置', '产品', '工作日', '燃料电池', '燃料', '范围', '税务总局', '条例', '财税', '材料', '动力'}\n"
     ]
    }
   ],
   "source": [
    "# graph.vocab\n",
    "# print(graph.sentence)\n",
    "H=graph.getMatrix(10)\n",
    "print(graph.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['产品', '信息化', '商用车', '工业', '新能源', '新能源-汽车', '新能源-汽车-产品', '新能源-汽车-标准', '新能源-船舶', '新能源-船舶-标准', '标准', '汽车', '目录', '船舶', '车船税']\n"
     ]
    }
   ],
   "source": [
    "rank,rankScore=PR(H)\n",
    "print(graph.rank(rank,10))\n",
    "# print(rankScore)\n",
    "#['产品', '信息化', '商用车', '工业', '新能源', '标准', '汽车', '目录', '船舶', '车船']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
