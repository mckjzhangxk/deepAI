{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First review the base knowage of linear Algebra\n",
    "\n",
    "<a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.eig.html\">eig</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.74406752  3.57594683  3.01381688  2.72441591  2.118274  ]\n",
      " [ 3.22947057  2.18793606  4.458865    4.8183138   1.91720759]\n",
      " [ 3.95862519  2.6444746   2.84022281  4.62798319  0.35518029]\n",
      " [ 0.4356465   0.10109199  4.16309923  3.89078375  4.35006074]\n",
      " [ 4.89309171  3.99579282  2.30739681  3.90264588  0.59137213]]\n",
      "------------------\n",
      "[[ 2.74406752  3.57594683  3.01381688  2.72441591  2.118274  ]\n",
      " [ 3.22947057  2.18793606  4.458865    4.8183138   1.91720759]\n",
      " [ 3.95862519  2.6444746   2.84022281  4.62798319  0.35518029]\n",
      " [ 0.4356465   0.10109199  4.16309923  3.89078375  4.35006074]\n",
      " [ 4.89309171  3.99579282  2.30739681  3.90264588  0.59137213]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "A=np.random.rand(5,5)*5;\n",
    "print(A)\n",
    "print(\"------------------\")\n",
    "W,S=np.linalg.eig(A)\n",
    "S_inv=np.linalg.inv(S)\n",
    "B=S.dot(np.diag(W)).dot(S_inv)\n",
    "print(B.real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next we implement PR algrithm\n",
    "formula: pr_new=(1-d)/n+d* (W*pr_old) <br />\n",
    "d is prob that a user keep stay at remain sites(prob a given vertex jump to another vertex)<br />\n",
    "W[i][j]=I{j-->i}/degree(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PR(W,prob=0.85,t=10):\n",
    "    '''\n",
    "        W:relation matrix,wij=I[j->i] / degree(j)\n",
    "        prob:prob jump from one to another vertex\n",
    "        t:number of iteration\n",
    "        return:rank,rank score,decent order\n",
    "    '''\n",
    "    \n",
    "    n=W.shape[0]\n",
    "    #initial uniformly\n",
    "    init_score=np.ones(n)/n\n",
    "    \n",
    "    E,S=np.linalg.eig(W)\n",
    "    S_inv=np.linalg.inv(S)\n",
    "    \n",
    "    #resolve or project initial score to eig space S\n",
    "    resolve=S_inv.dot(init_score)\n",
    "    \n",
    "    score=np.real(S.dot(np.diag(E**t)).dot(resolve))\n",
    "    score=prob*score+(1-prob)/n\n",
    "    \n",
    "    \n",
    "    rank=np.argsort(-score)\n",
    "    rankScore=score[rank]\n",
    "    return rank,rankScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example from this video <a href='https://www.youtube.com/watch?v=P8Kt6Abq_rM'>video</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W=np.array([\n",
    "        [0,0,1/3,0],\n",
    "        [1/2,0,1/3,0],\n",
    "        [1/2,0,0,1],\n",
    "        [0,1,1/3,0]\n",
    "    ])\n",
    "PR(W,prob=1,t=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Now review some NLTK \n",
    "## TOKENIZE\n",
    "## Corpara: body of text\n",
    "## Lexicon:the mean of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize,word_tokenize\n",
    "#unfortunally this fail to tokenize chinere,using jieba instead\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msg=\"黄晓明工作室对此发声明否认，称黄晓明不认识高某，未参与过任何操纵股票的行为；黄晓明未曾受过任何与股票有关的处罚，亦未介入过任何与股票有关的调查。黄晓明工作室还敦促相关方即刻删撤一切虚假信息，以免对公众造成误导并导致己方法律责任的承担。返回搜狐\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(jieba.cut(msg,cut_all=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'msg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-e1827508ef5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mchar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpseg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#     if pos.startswith('n') or pos.startswith('a'):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word %s,pos:%s'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#         words.append(char)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'msg' is not defined"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "for char,pos in pseg.cut(msg):\n",
    "#     if pos.startswith('n') or pos.startswith('a'):\n",
    "    print('word %s,pos:%s'%(char,pos))\n",
    "#         words.append(char)\n",
    "words=set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some document about jieba <a href='https://github.com/fxsjy/jieba'>doc</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corpora2Sentence(corpos):\n",
    "    '''\n",
    "        cut corpos into sentences\n",
    "        return:\n",
    "            sentences:list,one element represent a none adj,select[i]=(word,pos) mean word as pos\n",
    "            vocabs:a set of all words occur in corpos\n",
    "            word_idx and idx_word:the map between idx and vocab,I using a matrix representation\n",
    "                for the graph\n",
    "            vocabs_position:a dict,the element look like: \n",
    "                V(i):[p1,p2,..pn],mean word V occur at positon p1,p2,..pn\n",
    "            position_vocabs:reverse of vocabs_position:\n",
    "                pi:V(j),mean at positon pi is word V(j)\n",
    "    '''\n",
    "    sentences=[]\n",
    "    vocabs=[]  #store all vocabulary\n",
    "    vocabs_position={}\n",
    "    word_idx=0 #the word position at corpos\n",
    "    for char,pos in pseg.cut(corpos):\n",
    "        word_idx+=1\n",
    "        if pos=='x':continue\n",
    "        elif (pos.startswith('n') or pos=='a' or pos.startswith('v'))and len(char)>1:\n",
    "            sentences.append((char,word_idx))\n",
    "            vocabs.append(char)\n",
    "            if char in vocabs_position:vlist=vocabs_position[char]\n",
    "            else:\n",
    "                vlist=[]\n",
    "                vocabs_position[char]=vlist\n",
    "            vlist.append(word_idx)\n",
    "        \n",
    "        \n",
    "    vocabs=set(vocabs)\n",
    "    word_idx={c:i for i,c in enumerate(vocabs)}\n",
    "    idx_word={i:c for i,c in enumerate(vocabs)}\n",
    "    \n",
    "    position_vocabs={}\n",
    "    for vocb,poslist in vocabs_position.items():\n",
    "        for p in poslist:position_vocabs[p]=vocb\n",
    "\n",
    "    return sentences,vocabs,word_idx,idx_word,vocabs_position,position_vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I need a little Graph Theory to handle the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[产品]-[]\n",
    "#[信息化]-[]\n",
    "#[商用车]-[]\n",
    "#[工业]-[]\n",
    "#[新能源]-[汽车,船舶]\n",
    "#[汽车]-[产品,标准]\n",
    "#[船舶]-[标准]\n",
    "#[标准]-[]\n",
    "#[目录]-[]\n",
    "#[车船税]-[]\n",
    "#output should be  信息化 商用车 工业 新能源-汽车-产品 新能源-汽车-标准 新能源-船舶-标准 目录 车船税"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['工业', '车船税', '目录', '新能源-汽车-产品', '新能源-汽车-标准', '新能源-船舶-标准', '商用车', '信息化']\n"
     ]
    }
   ],
   "source": [
    "class Vertex():\n",
    "    '''\n",
    "        I using adjacency list representations,a convinient way to do DFS\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,w):\n",
    "        self.content=w\n",
    "        self.inDegree=0\n",
    "        self.adj=set() #adjacentcy vertexs\n",
    "    def __eq__(self,other):\n",
    "        return self.content==other.content\n",
    "    def __repr__(self):\n",
    "        return self.content\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    def __hash__(self):\n",
    "        return hash(self.content)\n",
    "class Vsets():\n",
    "    def __init__(self,words):\n",
    "        '''\n",
    "            maintain a set contain all the vertex\n",
    "        '''\n",
    "        self.vertexes={w:Vertex(w) for w in words}\n",
    "    def connect(self,v1,v2):\n",
    "        '''\n",
    "            add a edge from v1->v2\n",
    "            v1,v2 are string(key of self.vertexes)\n",
    "        '''\n",
    "        v1=self.vertexes[v1]\n",
    "        v2=self.vertexes[v2]\n",
    "        v1.adj.add(v2)\n",
    "        v2.inDegree+=1\n",
    "    def DFS(self):\n",
    "        '''\n",
    "            Do Depth First Serch on vertexes,start point is all vertex that\n",
    "            have inDegree of 0,in a other word,it's the root of tree\n",
    "            \n",
    "            return: a list of all the possible combination of content from root to leaf\n",
    "        '''\n",
    "        \n",
    "        #a helper dict member all visited vertex,initial state:no one be visited\n",
    "        self.visited={}\n",
    "        #record all vertex have in degress 0\n",
    "        root=set()\n",
    "        for w,v in self.vertexes.items():\n",
    "            self.visited[v]=False\n",
    "            if v.inDegree==0:\n",
    "                root.add(v)\n",
    "                \n",
    "                \n",
    "        \n",
    "        ret=[]\n",
    "        #do DFS_visit from root\n",
    "        for v in root:\n",
    "            v_list=self.DFS_Visit(v)\n",
    "            ret.extend(v_list)\n",
    "        del self.visited\n",
    "        return ret\n",
    "    def DFS_Visit(self,s):\n",
    "        '''\n",
    "            start from s,DFS to leaf\n",
    "            return: a list of all the possible combination of content from s to it's leaf\n",
    "        '''\n",
    "        self.visited[s]=True #first label this a visited vertex\n",
    "        ret=[]\n",
    "        \n",
    "        \n",
    "        for u in s.adj:\n",
    "            if self.visited[u]==False:\n",
    "                u_list=self.DFS_Visit(u)  #a list of all possible from u,do DFS_VISIT\n",
    "            else:\n",
    "                u_list=self.visited[u] #no need search \n",
    "            #append prefix s.content to subTree\n",
    "            ret.extend([s.content+'-'+c for c in u_list])\n",
    "        \n",
    "        #if s have no subTree to visited\n",
    "        if len(ret)==0:ret.append(s.content)\n",
    "        self.visited[s]=ret\n",
    "        return sorted(ret)\n",
    "#[产品]-[]\n",
    "#[信息化]-[]\n",
    "#[商用车]-[]\n",
    "#[工业]-[]\n",
    "#[新能源]-[汽车,船舶]\n",
    "#[汽车]-[产品,标准]\n",
    "#[船舶]-[标准]\n",
    "#[标准]-[]\n",
    "#[目录]-[]\n",
    "#[车船税]-[]\n",
    "#output should be  信息化 商用车 工业 目录 车船税 新能源-汽车-产品 新能源-汽车-标准 新能源-船舶-标准 \n",
    "\n",
    "vset=Vsets(['产品','信息化','商用车','工业','新能源','汽车','船舶','标准','目录','车船税'])\n",
    "vset.connect('新能源','汽车')\n",
    "vset.connect('新能源','船舶')\n",
    "vset.connect('汽车','产品')\n",
    "vset.connect('汽车','标准')\n",
    "vset.connect('船舶','标准')\n",
    "# vset.connect('新能源','汽车')\n",
    "print(vset.DFS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Sequence Graph Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    def __init__(self,filename):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        #cut corpus to words,remain only none and adjective\n",
    "        with open(filename) as f:\n",
    "            corpora=f.read()\n",
    "            self.sentence,self.vocab,self.word_idx,self.idx_word,self.vocabs_position,self.position_vocabs=corpora2Sentence(corpora)\n",
    "        \n",
    "        self.words=len(self.vocab)\n",
    "        self.H=np.zeros((self.words,self.words)) #matrix representation\n",
    "        \n",
    "    def _connect(self,char1,char2):\n",
    "        '''\n",
    "            char1,char2 is select 2 words by jieba,\n",
    "            char2 is after char2,\n",
    "            if the distance between char1,char2 is less than window_size,\n",
    "            I treat this is a valid edge,then update graph matrix\n",
    "            \n",
    "            char1:tuple(word1,position1)\n",
    "            \n",
    "            return True/False\n",
    "        '''\n",
    "        c1,p1,c2,p2=*char1,*char2\n",
    "        \n",
    "        if(abs(p2-p1)>self.windowSize):return False\n",
    "        idx1,idx2=self.word_idx[c1],self.word_idx[c2]\n",
    "        if idx1!=idx2:self.H[idx1,idx2]=self.H[idx2,idx1]=1\n",
    "        return True\n",
    "    def getMatrix(self,windowSize=5,eps=1e-6):\n",
    "        '''\n",
    "            compute graph matrix,using by Page Rank \n",
    "            windowsSize: within windowsSize ragime of word s view as neibourhood of s\n",
    "        '''\n",
    "        self.windowSize=windowSize\n",
    "        \n",
    "        sentencelen=len(self.sentence)\n",
    "        for i in range(sentencelen):\n",
    "            char1=self.sentence[i]\n",
    "            for j in range(i+1,sentencelen):\n",
    "                char2=self.sentence[j]\n",
    "                if self._connect(char1,char2)==False:break\n",
    "        \n",
    "        degree=np.sum(self.H,axis=0)\n",
    "        degree[degree==0]=eps\n",
    "        H=self.H/degree\n",
    "        return H\n",
    "    def rank(self,indices,top=1000):\n",
    "        '''\n",
    "            indices:the word's index according to Page Rank Algrithm,in decent order\n",
    "            top:select first top word of indices\n",
    "        '''\n",
    "\n",
    "        \n",
    "        selectedWords=[]\n",
    "        for i in range(min(len(indices),top)):\n",
    "            selectedWords.append(self.idx_word[indices[i]])\n",
    "        #using graph tool to handle the output\n",
    "        vstool=Vsets(selectedWords)\n",
    "        \n",
    "        #sort all positon of selected word occur in corpos\n",
    "        sel_pos_list=[]\n",
    "        for w in selectedWords:\n",
    "            sel_pos_list.extend(self.vocabs_position[w])\n",
    "        #sel_pos_list is a list of selectWords's positon ,for example,个税 is a select word,\n",
    "        #occur at position 10,30(reference to corpos),sel_pos_list look like [....10...30.....]\n",
    "        sel_pos_list=sorted(sel_pos_list) \n",
    "        \n",
    "        \n",
    "        '''\n",
    "            no go through sel_pos_list, if sel_pos_list[i] sel_pos_list[i+1] are continue posion ,\n",
    "            that mean W[sel_pos_list[i]] and W[sel_pos_list[i+1]] should merged as one word,\n",
    "            with vstool to add a edge from  W[sel_pos_list[i]] to W[sel_pos_list[i+1]],\n",
    "            vstool can handle consitive word properly!\n",
    "        \n",
    "        '''\n",
    "        for i in range(len(sel_pos_list)-1):\n",
    "            pi=sel_pos_list[i]\n",
    "            Wi=self.position_vocabs[pi]\n",
    "            pj=sel_pos_list[i+1]\n",
    "            Wj=self.position_vocabs[pj]\n",
    "            if(pj-pi==1 and Wi !=Wj):\n",
    "                vstool.connect(Wi,Wj)\n",
    "        ret=vstool.DFS()\n",
    "        return ret,sel_pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph=Graph('PR/tx1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'使用', '培训', '视同', '电动', '进口', '机构', '节约能源', '引燃', '方式', '是否', '含非', '装备', '减半', '办税', '具体', '天然气', '调整', '信息化', '规定', '附件', '船用', '行政监察', '汽油', '骗取', '继续', '虚假', '工作', '管理', '移送', '中华人民共和国', '材料', '确认', '检查', '样本', '动力', '宣传', '处理', '相应', '推进', '结果', '工作人员', '处分', '标准', '要求', '资格', '字段', '予以', '方面', '许可', '监测', '证书', '发布', '程式', '服务', '联合', '产品质量', '产量', '网站', '撤销', '检测', '车船税', '符合标准', '批准', '性能指标', '全部', '双燃料', '免征', '所有人', '新能源', '产业', '商用车', '气动力', '负责', '满足', '经销商', '促进', '列入', '执行', '提供', '申报', '标注', '比例', '真实性', '技术', '公告', '中国', '征收', '管理工作', '发展', '检验', '信息', '进口量', '申报材料', '柴油', '相关', '之日起', '两用', '公务员法', '通知', '燃油', '司法机关', '超过', '工况', '利用', '办公厅', '审核', '委托', '开展', '异议', '回收', '标记', '燃料电池', '提交', '加强', '节能', '纳税人', '中心', '资料', '动力装置', '免税', '核定', '提出', '国家税务总局', '税务总局', '年度', '因素', '国务院', '车船', '一致性', '范围', '国家', '管理系统', '减免', '财政部', '取得', '优惠', '车型', '组织', '涉嫌犯罪', '工业', '排量', '目录', '有关', '没有', '部门', '公示', '达到', '简称', '微量', '存在', '混合', '专项', '技术标准', '船舶', '综合', '认定', '采用', '符合', '优惠政策', '转让', '实施', '条例', '支撑', '电式', '财税', '监督', '财政', '工作日', '追究', '消耗量', '企业', '过程', '审查', '废止', '汽车', '燃料', '检验证书', '产品', '认为', '责任', '销售', '申请', '保证', '报告', '生产', '不符', '鼓励', '征税', '发动机', '获得', '属于', '燃用', '处罚', '建造', '现场', '法律法规', '动力电池', '乘用车', '热值', '享受'}\n"
     ]
    }
   ],
   "source": [
    "H=graph.getMatrix(10)\n",
    "print(graph.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['工业', '通知-新能源-汽车-标准', '通知-新能源-船舶-标准', '节能-汽车-标准', '车船税', '目录', '信息化']\n"
     ]
    }
   ],
   "source": [
    "rank,rankScore=PR(H)\n",
    "print()\n",
    "# print(rankScore)\n",
    "#['产品', '信息化', '商用车', '工业', '新能源', '标准', '汽车', '目录', '船舶', '车船税']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
