{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First review the base knowage of linear Algebra\n",
    "\n",
    "<a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.eig.html\">eig</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.74406752  3.57594683  3.01381688  2.72441591  2.118274  ]\n",
      " [ 3.22947057  2.18793606  4.458865    4.8183138   1.91720759]\n",
      " [ 3.95862519  2.6444746   2.84022281  4.62798319  0.35518029]\n",
      " [ 0.4356465   0.10109199  4.16309923  3.89078375  4.35006074]\n",
      " [ 4.89309171  3.99579282  2.30739681  3.90264588  0.59137213]]\n",
      "------------------\n",
      "[[ 2.74406752  3.57594683  3.01381688  2.72441591  2.118274  ]\n",
      " [ 3.22947057  2.18793606  4.458865    4.8183138   1.91720759]\n",
      " [ 3.95862519  2.6444746   2.84022281  4.62798319  0.35518029]\n",
      " [ 0.4356465   0.10109199  4.16309923  3.89078375  4.35006074]\n",
      " [ 4.89309171  3.99579282  2.30739681  3.90264588  0.59137213]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "A=np.random.rand(5,5)*5;\n",
    "print(A)\n",
    "print(\"------------------\")\n",
    "W,S=np.linalg.eig(A)\n",
    "S_inv=np.linalg.inv(S)\n",
    "B=S.dot(np.diag(W)).dot(S_inv)\n",
    "print(B.real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next we implement PR algrithm\n",
    "formula: pr_new=(1-d)/n+d* (W*pr_old) <br />\n",
    "d is prob that a user keep stay at remain sites(prob a given vertex jump to another vertex)<br />\n",
    "W[i][j]=I{j-->i}/degree(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PR(W,prob=0.85,t=10):\n",
    "    '''\n",
    "        W:relation matrix,wij=I[j->i] / degree(j)\n",
    "        prob:prob jump from one to another vertex\n",
    "        t:number of iteration\n",
    "        return:rank,rank score,decent order\n",
    "    '''\n",
    "    \n",
    "    n=W.shape[0]\n",
    "    #initial uniformly\n",
    "    init_score=np.ones(n)/n\n",
    "    \n",
    "    E,S=np.linalg.eig(W)\n",
    "    S_inv=np.linalg.inv(S)\n",
    "    \n",
    "    #resolve or project initial score to eig space S\n",
    "    resolve=S_inv.dot(init_score)\n",
    "    \n",
    "    score=np.real(S.dot(np.diag(E**t)).dot(resolve))\n",
    "    score=prob*score+(1-prob)/n\n",
    "    \n",
    "    \n",
    "    rank=np.argsort(-score)\n",
    "    rankScore=score[rank]\n",
    "    return rank,rankScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example from this video <a href='https://www.youtube.com/watch?v=P8Kt6Abq_rM'>video</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W=np.array([\n",
    "        [0,0,1/3,0],\n",
    "        [1/2,0,1/3,0],\n",
    "        [1/2,0,0,1],\n",
    "        [0,1,1/3,0]\n",
    "    ])\n",
    "PR(W,prob=1,t=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Now review some NLTK \n",
    "## TOKENIZE\n",
    "## Corpara: body of text\n",
    "## Lexicon:the mean of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize,word_tokenize\n",
    "#unfortunally this fail to tokenize chinere,using jieba instead\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msg=\"黄晓明工作室对此发声明否认，称黄晓明不认识高某，未参与过任何操纵股票的行为；黄晓明未曾受过任何与股票有关的处罚，亦未介入过任何与股票有关的调查。黄晓明工作室还敦促相关方即刻删撤一切虚假信息，以免对公众造成误导并导致己方法律责任的承担。返回搜狐\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(jieba.cut(msg,cut_all=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'msg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-e1827508ef5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mchar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpseg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#     if pos.startswith('n') or pos.startswith('a'):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'word %s,pos:%s'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#         words.append(char)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'msg' is not defined"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "for char,pos in pseg.cut(msg):\n",
    "#     if pos.startswith('n') or pos.startswith('a'):\n",
    "    print('word %s,pos:%s'%(char,pos))\n",
    "#         words.append(char)\n",
    "words=set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some document about jieba <a href='https://github.com/fxsjy/jieba'>doc</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corpora2Sentence(corpos):\n",
    "    '''\n",
    "        cut corpos into sentences\n",
    "        return:\n",
    "            sentences:list,one element represent a none adj,select[i]=(word,pos) mean word as pos\n",
    "            vocabs:a set of all words occur in corpos\n",
    "            word_idx and idx_word:the map between idx and vocab,I using a matrix representation\n",
    "                for the graph\n",
    "            vocabs_position:a dict,the element look like: \n",
    "                V(i):[p1,p2,..pn],mean word V occur at positon p1,p2,..pn\n",
    "            position_vocabs:reverse of vocabs_position:\n",
    "                pi:V(j),mean at positon pi is word V(j)\n",
    "    '''\n",
    "    sentences=[]\n",
    "    vocabs=[]  #store all vocabulary\n",
    "    vocabs_position={}\n",
    "    word_idx=0 #the word position at corpos\n",
    "    for char,pos in pseg.cut(corpos):\n",
    "        word_idx+=1\n",
    "        if pos=='x':continue\n",
    "        elif (pos.startswith('n') or pos=='a' or pos.startswith('v'))and len(char)>1:\n",
    "            sentences.append((char,word_idx))\n",
    "            vocabs.append(char)\n",
    "            if char in vocabs_position:vlist=vocabs_position[char]\n",
    "            else:\n",
    "                vlist=[]\n",
    "                vocabs_position[char]=vlist\n",
    "            vlist.append(word_idx)\n",
    "        \n",
    "        \n",
    "    vocabs=set(vocabs)\n",
    "    word_idx={c:i for i,c in enumerate(vocabs)}\n",
    "    idx_word={i:c for i,c in enumerate(vocabs)}\n",
    "    \n",
    "    position_vocabs={}\n",
    "    for vocb,poslist in vocabs_position.items():\n",
    "        for p in poslist:position_vocabs[p]=vocb\n",
    "\n",
    "    return sentences,vocabs,word_idx,idx_word,vocabs_position,position_vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I need a little Graph Theory to handle the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[产品]-[]\n",
    "#[信息化]-[]\n",
    "#[商用车]-[]\n",
    "#[工业]-[]\n",
    "#[新能源]-[汽车,船舶]\n",
    "#[汽车]-[产品,标准]\n",
    "#[船舶]-[标准]\n",
    "#[标准]-[]\n",
    "#[目录]-[]\n",
    "#[车船税]-[]\n",
    "#output should be  信息化 商用车 工业 新能源-汽车-产品 新能源-汽车-标准 新能源-船舶-标准 目录 车船税"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['工业', '新能源汽车产品', '新能源汽车标准', '新能源船舶标准', '商用车', '目录', '信息化', '车船税']\n"
     ]
    }
   ],
   "source": [
    "class Vertex():\n",
    "    '''\n",
    "        I using adjacency list representations,a convinient way to do DFS\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,w):\n",
    "        self.content=w\n",
    "        self.inDegree=0\n",
    "        self.adj=set() #adjacentcy vertexs\n",
    "    def __eq__(self,other):\n",
    "        return self.content==other.content\n",
    "    def __repr__(self):\n",
    "        return self.content\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    def __hash__(self):\n",
    "        return hash(self.content)\n",
    "class Vsets():\n",
    "    def __init__(self,words):\n",
    "        '''\n",
    "            maintain a set contain all the vertex\n",
    "        '''\n",
    "        self.vertexes={w:Vertex(w) for w in words}\n",
    "    def connect(self,v1,v2):\n",
    "        '''\n",
    "            add a edge from v1->v2\n",
    "            v1,v2 are string(key of self.vertexes)\n",
    "        '''\n",
    "        v1=self.vertexes[v1]\n",
    "        v2=self.vertexes[v2]\n",
    "        v1.adj.add(v2)\n",
    "        v2.inDegree+=1\n",
    "    def DFS(self):\n",
    "        '''\n",
    "            Do Depth First Serch on vertexes,start point is all vertex that\n",
    "            have inDegree of 0,in a other word,it's the root of tree\n",
    "            \n",
    "            return: a list of all the possible combination of content from root to leaf\n",
    "        '''\n",
    "        \n",
    "        #a helper dict member all visited vertex,initial state:no one be visited\n",
    "        self.visited={}\n",
    "        #record all vertex have in degress 0\n",
    "        root=set()\n",
    "        for w,v in self.vertexes.items():\n",
    "            self.visited[v]=False\n",
    "            if v.inDegree==0:\n",
    "                root.add(v)\n",
    "                \n",
    "                \n",
    "        \n",
    "        ret=[]\n",
    "        #do DFS_visit from root\n",
    "        for v in root:\n",
    "            v_list=self.DFS_Visit(v)\n",
    "            ret.extend(v_list)\n",
    "        del self.visited\n",
    "        return ret\n",
    "    def DFS_Visit(self,s):\n",
    "        '''\n",
    "            start from s,DFS to leaf\n",
    "            return: a list of all the possible combination of content from s to it's leaf\n",
    "        '''\n",
    "        self.visited[s]=True #first label this a visited vertex\n",
    "        ret=[]\n",
    "        \n",
    "        \n",
    "        for u in s.adj:\n",
    "            if self.visited[u]==False:\n",
    "                u_list=self.DFS_Visit(u)  #a list of all possible from u,do DFS_VISIT\n",
    "            else:\n",
    "                u_list=self.visited[u] #no need search \n",
    "            #append prefix s.content to subTree\n",
    "            ret.extend([s.content+c for c in u_list])\n",
    "        \n",
    "        #if s have no subTree to visited\n",
    "        if len(ret)==0:ret.append(s.content)\n",
    "        self.visited[s]=ret\n",
    "        return sorted(ret)\n",
    "#[产品]-[]\n",
    "#[信息化]-[]\n",
    "#[商用车]-[]\n",
    "#[工业]-[]\n",
    "#[新能源]-[汽车,船舶]\n",
    "#[汽车]-[产品,标准]\n",
    "#[船舶]-[标准]\n",
    "#[标准]-[]\n",
    "#[目录]-[]\n",
    "#[车船税]-[]\n",
    "#output should be  信息化 商用车 工业 目录 车船税 新能源-汽车-产品 新能源-汽车-标准 新能源-船舶-标准 \n",
    "\n",
    "vset=Vsets(['产品','信息化','商用车','工业','新能源','汽车','船舶','标准','目录','车船税'])\n",
    "vset.connect('新能源','汽车')\n",
    "vset.connect('新能源','船舶')\n",
    "vset.connect('汽车','产品')\n",
    "vset.connect('汽车','标准')\n",
    "vset.connect('船舶','标准')\n",
    "# vset.connect('新能源','汽车')\n",
    "print(vset.DFS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Sequence Graph Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    def __init__(self,filename):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        #cut corpus to words,remain only none and adjective\n",
    "        with open(filename) as f:\n",
    "            corpora=f.read()\n",
    "            self.sentence,self.vocab,self.word_idx,self.idx_word,self.vocabs_position,self.position_vocabs=corpora2Sentence(corpora)\n",
    "        \n",
    "        self.words=len(self.vocab)\n",
    "        self.H=np.zeros((self.words,self.words)) #matrix representation\n",
    "        \n",
    "    def _connect(self,char1,char2):\n",
    "        '''\n",
    "            char1,char2 is select 2 words by jieba,\n",
    "            char2 is after char2,\n",
    "            if the distance between char1,char2 is less than window_size,\n",
    "            I treat this is a valid edge,then update graph matrix\n",
    "            \n",
    "            char1:tuple(word1,position1)\n",
    "            \n",
    "            return True/False\n",
    "        '''\n",
    "        c1,p1,c2,p2=*char1,*char2\n",
    "        \n",
    "        if(abs(p2-p1)>self.windowSize):return False\n",
    "        idx1,idx2=self.word_idx[c1],self.word_idx[c2]\n",
    "        if idx1!=idx2:self.H[idx1,idx2]=self.H[idx2,idx1]=1\n",
    "        return True\n",
    "    def getMatrix(self,windowSize=5,eps=1e-6):\n",
    "        '''\n",
    "            compute graph matrix,using by Page Rank \n",
    "            windowsSize: within windowsSize ragime of word s view as neibourhood of s\n",
    "        '''\n",
    "        self.windowSize=windowSize\n",
    "        \n",
    "        sentencelen=len(self.sentence)\n",
    "        for i in range(sentencelen):\n",
    "            char1=self.sentence[i]\n",
    "            for j in range(i+1,sentencelen):\n",
    "                char2=self.sentence[j]\n",
    "                if self._connect(char1,char2)==False:break\n",
    "        \n",
    "        degree=np.sum(self.H,axis=0)\n",
    "        degree[degree==0]=eps\n",
    "        H=self.H/degree\n",
    "        return H\n",
    "    def rank(self,indices,top=1000):\n",
    "        '''\n",
    "            indices:the word's index according to Page Rank Algrithm,in decent order\n",
    "            top:select first top word of indices\n",
    "        '''\n",
    "\n",
    "        \n",
    "        selectedWords=[]\n",
    "        for i in range(min(len(indices),top)):\n",
    "            selectedWords.append(self.idx_word[indices[i]])\n",
    "        #using graph tool to handle the output\n",
    "        vstool=Vsets(selectedWords)\n",
    "        \n",
    "        #sort all positon of selected word occur in corpos\n",
    "        sel_pos_list=[]\n",
    "        for w in selectedWords:\n",
    "            sel_pos_list.extend(self.vocabs_position[w])\n",
    "        #sel_pos_list is a list of selectWords's positon ,for example,个税 is a select word,\n",
    "        #occur at position 10,30(reference to corpos),sel_pos_list look like [....10...30.....]\n",
    "        sel_pos_list=sorted(sel_pos_list) \n",
    "        \n",
    "        \n",
    "        '''\n",
    "            no go through sel_pos_list, if sel_pos_list[i] sel_pos_list[i+1] are continue posion ,\n",
    "            that mean W[sel_pos_list[i]] and W[sel_pos_list[i+1]] should merged as one word,\n",
    "            with vstool to add a edge from  W[sel_pos_list[i]] to W[sel_pos_list[i+1]],\n",
    "            vstool can handle consitive word properly!\n",
    "        \n",
    "        '''\n",
    "        for i in range(len(sel_pos_list)-1):\n",
    "            pi=sel_pos_list[i]\n",
    "            Wi=self.position_vocabs[pi]\n",
    "            pj=sel_pos_list[i+1]\n",
    "            Wj=self.position_vocabs[pj]\n",
    "            if(pj-pi==1 and Wi !=Wj):\n",
    "                vstool.connect(Wi,Wj)\n",
    "        ret=vstool.DFS()\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph=Graph('tx4.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'大众', '退税', '新华社', '公众', '对象', '退库', '教学大纲', '具有', '缴纳', '倡导', '广西', '财政部', '小学', '传媒', '进行', '包括', '机关', '有关', '财税', '图书', '通知', '零售', '简称', '后退', '军事', '教育', '自治区', '免税', '出现', '职业高中', '国家税务总局', '软盘', '少数民族', '注释', '具体', '掌握', '专业', '出版发行', '详见', '任务', '基地', '简并', '取得', '报纸', '含以', '财政', '行政', '政策', '指以', '辅导', '科学', '主管', '许可证', '宁夏', '传播', '期刊', '审定', '印刷', '办事处', '列入', '相应', '核算', '光盘', '制作', '部门', '开放', '参考书', '共青团', '相关', '申请', '环节', '印发', '目录', '民主党派', '利用', '延续', '政府', '练习册', '参与', '新疆维吾尔自治区', '载体', '课外读物', '营业税', '组织', '党政部门', '信息', '出版单位', '增值税', '主管部门', '名单', '分离', '优惠政策', '批准', '预字', '地震台', '批发', '追回', '方式', '理解', '规定', '变更', '办理', '内蒙古', '新疆', '注册', '普通', '残联', '知识', '行政部门', '出版物', '租型', '纳税人', '附件', '门票', '改革', '征税', '教材', '弘扬', '中等职业', '业务', '活动', '形式', '照章', '监察', '原因', '税收', '财务', '实行', '问题', '自读', '税务机关', '货物', '重组', '废止', '无法', '定义', '单位', '核准', '图册', '出版', '专员', '下列', '科学技术', '免征', '成人', '部分', '学生上课', '中国共产党', '工会', '享受', '博物馆', '征收', '起至', '管理', '入库', '开具', '电子', '税率', '盲文', '应予', '杂志', '认定', '方法', '开展', '使用', '预算', '课本', '收入', '天文馆', '下达', '读物', '接到', '国务院', '发行', '科研机构', '学生', '月份', '科普', '可抵', '范围', '科普活动', '国家', '提供', '接受', '思想', '专用发票', '税务总局', '资质', '购买方', '先征', '指定', '音像制品', '磁带', '执行', '改制', '不得', '社会保障', '气象台', '推广', '教科书', '违规', '辅助性', '中国人民银行', '老年人', '社会科学', '科技馆', '易于', '税款', '浅显', '人力资源', '介绍', '文化', '承担', '要求', '用书', '宣传', '文字', '西藏', '出版权', '学校', '企业', '教育部', '税制', '应用', '精神', '销售', '教学'}\n"
     ]
    }
   ],
   "source": [
    "H=graph.getMatrix(10)\n",
    "print(graph.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['课本', '单位', '报纸', '出版物出版', '通知', '期刊增值税先征后退']\n"
     ]
    }
   ],
   "source": [
    "rank,rankScore=PR(H)\n",
    "print(graph.rank(rank,10))\n",
    "# print(rankScore)\n",
    "#['产品', '信息化', '商用车', '工业', '新能源', '标准', '汽车', '目录', '船舶', '车船税']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
