{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First review the base knowage of linear Algebra\n",
    "\n",
    "<a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.eig.html\">eig</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "A=np.random.rand(5,5)*5;\n",
    "print(A)\n",
    "print(\"------------------\")\n",
    "W,S=np.linalg.eig(A)\n",
    "S_inv=np.linalg.inv(S)\n",
    "B=S.dot(np.diag(W)).dot(S_inv)\n",
    "print(B.real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next we implement PR algrithm\n",
    "formula: pr_new=(1-d)/n+d* (W*pr_old) <br />\n",
    "d is prob that a user keep stay at remain sites(prob a given vertex jump to another vertex)<br />\n",
    "W[i][j]=I{j-->i}/degree(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PR(W,prob=0.85,t=10):\n",
    "    '''\n",
    "        W:relation matrix,wij=I[j->i] / degree(j)\n",
    "        prob:prob jump from one to another vertex\n",
    "        t:number of iteration\n",
    "        return:rank,rank score,decent order\n",
    "    '''\n",
    "    \n",
    "    n=W.shape[0]\n",
    "    #initial uniformly\n",
    "    init_score=np.ones(n)/n\n",
    "    \n",
    "    E,S=np.linalg.eig(W)\n",
    "    S_inv=np.linalg.inv(S)\n",
    "    \n",
    "    #resolve or project initial score to eig space S\n",
    "    resolve=S_inv.dot(init_score)\n",
    "    \n",
    "    score=np.real(S.dot(np.diag(E**t)).dot(resolve))\n",
    "    score=prob*score+(1-prob)/n\n",
    "    \n",
    "    \n",
    "    rank=np.argsort(-score)\n",
    "    rankScore=score[rank]\n",
    "    return rank,rankScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example from this video <a href='https://www.youtube.com/watch?v=P8Kt6Abq_rM'>video</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W=np.array([\n",
    "        [0,0,1/3,0],\n",
    "        [1/2,0,1/3,0],\n",
    "        [1/2,0,0,1],\n",
    "        [0,1,1/3,0]\n",
    "    ])\n",
    "PR(W,prob=1,t=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Now review some NLTK \n",
    "## TOKENIZE\n",
    "## Corpara: body of text\n",
    "## Lexicon:the mean of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize,word_tokenize\n",
    "#unfortunally this fail to tokenize chinere,using jieba instead\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msg=\"黄晓明工作室对此发声明否认，称黄晓明不认识高某，未参与过任何操纵股票的行为；黄晓明未曾受过任何与股票有关的处罚，亦未介入过任何与股票有关的调查。黄晓明工作室还敦促相关方即刻删撤一切虚假信息，以免对公众造成误导并导致己方法律责任的承担。返回搜狐\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(jieba.cut(msg,cut_all=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words=[]\n",
    "for char,pos in pseg.cut(msg):\n",
    "#     if pos.startswith('n') or pos.startswith('a'):\n",
    "    print('word %s,pos:%s'%(char,pos))\n",
    "#         words.append(char)\n",
    "words=set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some document about jieba <a href='https://github.com/fxsjy/jieba'>doc</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corpora2Sentence(corpos):\n",
    "    '''\n",
    "        cut corpos into sentences\n",
    "        return:\n",
    "            sentences:list,one element represent a none adj,select[i]=(word,pos) mean word as pos\n",
    "            vocabs:a set of all words occur in corpos\n",
    "            word_idx and idx_word:the map between idx and vocab,I using a matrix representation\n",
    "                for the graph\n",
    "            vocabs_position:a dict,the element look like: \n",
    "                V(i):[p1,p2,..pn],mean word V occur at positon p1,p2,..pn\n",
    "            position_vocabs:reverse of vocabs_position:\n",
    "                pi:V(j),mean at positon pi is word V(j)\n",
    "    '''\n",
    "    sentences=[]\n",
    "    vocabs=[]  #store all vocabulary\n",
    "    vocabs_position={}\n",
    "    word_idx=0 #the word position at corpos\n",
    "    for char,pos in pseg.cut(corpos):\n",
    "        word_idx+=1\n",
    "        if pos=='x':continue\n",
    "        elif (pos.startswith('n') or pos=='a')and len(char)>1:\n",
    "            sentences.append((char,word_idx))\n",
    "            vocabs.append(char)\n",
    "            if char in vocabs_position:vlist=vocabs_position[char]\n",
    "            else:\n",
    "                vlist=[]\n",
    "                vocabs_position[char]=vlist\n",
    "            vlist.append(word_idx)\n",
    "        \n",
    "        \n",
    "    vocabs=set(vocabs)\n",
    "    word_idx={c:i for i,c in enumerate(vocabs)}\n",
    "    idx_word={i:c for i,c in enumerate(vocabs)}\n",
    "    \n",
    "    position_vocabs={}\n",
    "    for vocb,poslist in vocabs_position.items():\n",
    "        for p in poslist:position_vocabs[p]=vocb\n",
    "\n",
    "    return sentences,vocabs,word_idx,idx_word,vocabs_position,position_vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I need a little Graph Theory to handle the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[产品]-[]\n",
    "#[信息化]-[]\n",
    "#[商用车]-[]\n",
    "#[工业]-[]\n",
    "#[新能源]-[汽车,船舶]\n",
    "#[汽车]-[产品,标准]\n",
    "#[船舶]-[标准]\n",
    "#[标准]-[]\n",
    "#[目录]-[]\n",
    "#[车船税]-[]\n",
    "#output should be  信息化 商用车 工业 新能源-汽车-产品 新能源-汽车-标准 新能源-船舶-标准 目录 车船税"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['信息化', '商用车', '目录', '工业', '车船税', '新能源汽车产品', '新能源汽车标准', '新能源船舶标准']\n"
     ]
    }
   ],
   "source": [
    "class Vertex():\n",
    "    '''\n",
    "        I using adjacency list representations,a convinient way to do DFS\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,w):\n",
    "        self.content=w\n",
    "        self.inDegree=0\n",
    "        self.adj=set() #adjacentcy vertexs\n",
    "    def __eq__(self,other):\n",
    "        return self.content==other.content\n",
    "    def __repr__(self):\n",
    "        return self.content\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    def __hash__(self):\n",
    "        return hash(self.content)\n",
    "class Vsets():\n",
    "    def __init__(self,words):\n",
    "        '''\n",
    "            maintain a set contain all the vertex\n",
    "        '''\n",
    "        self.vertexes={w:Vertex(w) for w in words}\n",
    "    def connect(self,v1,v2):\n",
    "        '''\n",
    "            add a edge from v1->v2\n",
    "            v1,v2 are string(key of self.vertexes)\n",
    "        '''\n",
    "        v1=self.vertexes[v1]\n",
    "        v2=self.vertexes[v2]\n",
    "        v1.adj.add(v2)\n",
    "        v2.inDegree+=1\n",
    "    def DFS(self):\n",
    "        '''\n",
    "            Do Depth First Serch on vertexes,start point is all vertex that\n",
    "            have inDegree of 0,in a other word,it's the root of tree\n",
    "            \n",
    "            return: a list of all the possible combination of content from root to leaf\n",
    "        '''\n",
    "        \n",
    "        #a helper dict member all visited vertex,initial state:no one be visited\n",
    "        self.visited={}\n",
    "        #record all vertex have in degress 0\n",
    "        root=set()\n",
    "        for w,v in self.vertexes.items():\n",
    "            self.visited[v]=False\n",
    "            if v.inDegree==0:\n",
    "                root.add(v)\n",
    "                \n",
    "                \n",
    "        \n",
    "        ret=[]\n",
    "        #do DFS_visit from root\n",
    "        for v in root:\n",
    "            v_list=self.DFS_Visit(v)\n",
    "            ret.extend(v_list)\n",
    "        del self.visited\n",
    "        return ret\n",
    "    def DFS_Visit(self,s):\n",
    "        '''\n",
    "            start from s,DFS to leaf\n",
    "            return: a list of all the possible combination of content from s to it's leaf\n",
    "        '''\n",
    "        self.visited[s]=True #first label this a visited vertex\n",
    "        ret=[]\n",
    "        \n",
    "        \n",
    "        for u in s.adj:\n",
    "            if self.visited[u]==False:\n",
    "                u_list=self.DFS_Visit(u)  #a list of all possible from u,do DFS_VISIT\n",
    "            else:\n",
    "                u_list=self.visited[u] #no need search \n",
    "            #append prefix s.content to subTree\n",
    "            ret.extend([s.content+c for c in u_list])\n",
    "        \n",
    "        #if s have no subTree to visited\n",
    "        if len(ret)==0:ret.append(s.content)\n",
    "        self.visited[s]=ret\n",
    "        return sorted(ret)\n",
    "#[产品]-[]\n",
    "#[信息化]-[]\n",
    "#[商用车]-[]\n",
    "#[工业]-[]\n",
    "#[新能源]-[汽车,船舶]\n",
    "#[汽车]-[产品,标准]\n",
    "#[船舶]-[标准]\n",
    "#[标准]-[]\n",
    "#[目录]-[]\n",
    "#[车船税]-[]\n",
    "#output should be  信息化 商用车 工业 目录 车船税 新能源-汽车-产品 新能源-汽车-标准 新能源-船舶-标准 \n",
    "\n",
    "vset=Vsets(['产品','信息化','商用车','工业','新能源','汽车','船舶','标准','目录','车船税'])\n",
    "vset.connect('新能源','汽车')\n",
    "vset.connect('新能源','船舶')\n",
    "vset.connect('汽车','产品')\n",
    "vset.connect('汽车','标准')\n",
    "vset.connect('船舶','标准')\n",
    "# vset.connect('新能源','汽车')\n",
    "print(vset.DFS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Sequence Graph Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    def __init__(self,filename):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        #cut corpus to words,remain only none and adjective\n",
    "        with open(filename) as f:\n",
    "            corpora=f.read()\n",
    "            self.sentence,self.vocab,self.word_idx,self.idx_word,self.vocabs_position,self.position_vocabs=corpora2Sentence(corpora)\n",
    "        \n",
    "        self.words=len(self.vocab)\n",
    "        self.H=np.zeros((self.words,self.words)) #matrix representation\n",
    "        \n",
    "    def _connect(self,char1,char2):\n",
    "        '''\n",
    "            char1,char2 is select 2 words by jieba,\n",
    "            char2 is after char2,\n",
    "            if the distance between char1,char2 is less than window_size,\n",
    "            I treat this is a valid edge,then update graph matrix\n",
    "            \n",
    "            char1:tuple(word1,position1)\n",
    "            \n",
    "            return True/False\n",
    "        '''\n",
    "        c1,p1,c2,p2=*char1,*char2\n",
    "        \n",
    "        if(abs(p2-p1)>self.windowSize):return False\n",
    "        idx1,idx2=self.word_idx[c1],self.word_idx[c2]\n",
    "        if idx1!=idx2:self.H[idx1,idx2]=self.H[idx2,idx1]=1\n",
    "        return True\n",
    "    def getMatrix(self,windowSize=5,eps=1e-6):\n",
    "        '''\n",
    "            compute graph matrix,using by Page Rank \n",
    "            windowsSize: within windowsSize ragime of word s view as neibourhood of s\n",
    "        '''\n",
    "        self.windowSize=windowSize\n",
    "        \n",
    "        sentencelen=len(self.sentence)\n",
    "        for i in range(sentencelen):\n",
    "            char1=self.sentence[i]\n",
    "            for j in range(i+1,sentencelen):\n",
    "                char2=self.sentence[j]\n",
    "                if self._connect(char1,char2)==False:break\n",
    "        \n",
    "        degree=np.sum(self.H,axis=0)\n",
    "        degree[degree==0]=eps\n",
    "        H=self.H/degree\n",
    "        return H\n",
    "    def rank(self,indices,top=1000):\n",
    "        '''\n",
    "            indices:the word's index according to Page Rank Algrithm,in decent order\n",
    "            top:select first top word of indices\n",
    "        '''\n",
    "\n",
    "        \n",
    "        selectedWords=[]\n",
    "        for i in range(min(len(indices),top)):\n",
    "            selectedWords.append(self.idx_word[indices[i]])\n",
    "        #using graph tool to handle the output\n",
    "        vstool=Vsets(selectedWords)\n",
    "        \n",
    "        #sort all positon of selected word occur in corpos\n",
    "        sel_pos_list=[]\n",
    "        for w in selectedWords:\n",
    "            sel_pos_list.extend(self.vocabs_position[w])\n",
    "        #sel_pos_list is a list of selectWords's positon ,for example,个税 is a select word,\n",
    "        #occur at position 10,30(reference to corpos),sel_pos_list look like [....10...30.....]\n",
    "        sel_pos_list=sorted(sel_pos_list) \n",
    "        \n",
    "        \n",
    "        '''\n",
    "            no go through sel_pos_list, if sel_pos_list[i] sel_pos_list[i+1] are continue posion ,\n",
    "            that mean W[sel_pos_list[i]] and W[sel_pos_list[i+1]] should merged as one word,\n",
    "            with vstool to add a edge from  W[sel_pos_list[i]] to W[sel_pos_list[i+1]],\n",
    "            vstool can handle consitive word properly!\n",
    "        \n",
    "        '''\n",
    "        for i in range(len(sel_pos_list)-1):\n",
    "            pi=sel_pos_list[i]\n",
    "            Wi=self.position_vocabs[pi]\n",
    "            pj=sel_pos_list[i+1]\n",
    "            Wj=self.position_vocabs[pj]\n",
    "            if(pj-pi==1 and Wi !=Wj):\n",
    "                vstool.connect(Wi,Wj)\n",
    "        ret=vstool.DFS()\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.822 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.822 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "graph=Graph('news.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'网站', '中心', '消耗量', '目录', '主管部门', '产业', '装备', '纳税人', '责任', '新疆生产建设兵团', '财政部', '两用', '船用', '国务院', '程式', '办税', '年度', '气动力', '产品', '工况', '进口量', '涉嫌犯罪', '乘用车', '所有人', '方式', '国家', '热值', '材料', '公告', '结果', '因素', '范围', '管理系统', '车型', '燃料电池', '中华人民共和国', '排量', '产品质量', '微量', '机构', '技术', '异议', '动力装置', '商用车', '柴油', '虚假', '行政监察', '天然气', '工业', '企业', '税务局', '财政厅', '全部', '技术标准', '办公厅', '税务总局', '电式', '工作人员', '财政', '方面', '中国', '核定', '新能源', '自治区', '利用', '节约能源', '信息', '燃油', '汽油', '财税', '视同', '车船税', '计划单列市', '船舶', '资格', '双燃料', '直辖市', '处分', '司法机关', '工作日', '申报材料', '燃用', '部门', '样本', '许可', '比例', '之日起', '委托', '产量', '字段', '汽车', '燃料', '标准', '性能指标', '优惠政策', '专项', '符合标准', '电动', '车船', '法律法规', '现场', '委员会', '国家税务总局', '报告', '检验证书', '公务员法', '申报', '发动机', '真实性', '财政局', '管理工作', '资料', '一致性', '规定', '动力电池', '动力', '附件', '过程', '条例', '经销商', '具体', '证书', '信息化', '标记'}\n"
     ]
    }
   ],
   "source": [
    "H=graph.getMatrix(10)\n",
    "print(graph.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['信息化', '商用车', '目录', '工业', '车船税', '新能源汽车产品', '新能源汽车标准', '新能源船舶标准']\n"
     ]
    }
   ],
   "source": [
    "rank,rankScore=PR(H)\n",
    "print(graph.rank(rank,10))\n",
    "# print(rankScore)\n",
    "#['产品', '信息化', '商用车', '工业', '新能源', '标准', '汽车', '目录', '船舶', '车船税']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
