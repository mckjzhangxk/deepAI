{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First review the base knowage of linear Algebra\n",
    "\n",
    "<a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.eig.html\">eig</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.74406752 3.57594683 3.01381688 2.72441591 2.118274  ]\n",
      " [3.22947057 2.18793606 4.458865   4.8183138  1.91720759]\n",
      " [3.95862519 2.6444746  2.84022281 4.62798319 0.35518029]\n",
      " [0.4356465  0.10109199 4.16309923 3.89078375 4.35006074]\n",
      " [4.89309171 3.99579282 2.30739681 3.90264588 0.59137213]]\n",
      "------------------\n",
      "[[2.74406752 3.57594683 3.01381688 2.72441591 2.118274  ]\n",
      " [3.22947057 2.18793606 4.458865   4.8183138  1.91720759]\n",
      " [3.95862519 2.6444746  2.84022281 4.62798319 0.35518029]\n",
      " [0.4356465  0.10109199 4.16309923 3.89078375 4.35006074]\n",
      " [4.89309171 3.99579282 2.30739681 3.90264588 0.59137213]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "A=np.random.rand(5,5)*5;\n",
    "print(A)\n",
    "print(\"------------------\")\n",
    "W,S=np.linalg.eig(A)\n",
    "S_inv=np.linalg.inv(S)\n",
    "B=S.dot(np.diag(W)).dot(S_inv)\n",
    "print(B.real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next we implement PR algrithm\n",
    "formula: pr_new=(1-d)/n+d* (W*pr_old) <br />\n",
    "d is prob that a user keep stay at remain sites(prob a given vertex jump to another vertex)<br />\n",
    "W[i][j]=I{j-->i}/degree(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PR(W,prob=0.85,t=10):\n",
    "    '''\n",
    "        W:relation matrix,wij=I[j->i] / degree(j)\n",
    "        prob:prob jump from one to another vertex\n",
    "        t:number of iteration\n",
    "        return:rank,rank score,decent order\n",
    "    '''\n",
    "    \n",
    "    n=W.shape[0]\n",
    "    #initial uniformly\n",
    "    init_score=np.ones(n)/n\n",
    "    \n",
    "    E,S=np.linalg.eig(W)\n",
    "    S_inv=np.linalg.inv(S)\n",
    "    \n",
    "    #resolve or project initial score to eig space S\n",
    "    resolve=S_inv.dot(init_score)\n",
    "    \n",
    "    score=np.real(S.dot(np.diag(E**t)).dot(resolve))\n",
    "    score=prob*score+(1-prob)/n\n",
    "    \n",
    "    \n",
    "    rank=np.argsort(-score)\n",
    "    rankScore=score[rank]\n",
    "    return rank,rankScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example from this video <a href='https://www.youtube.com/watch?v=P8Kt6Abq_rM'>video</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 3, 1, 0]), array([0.375 , 0.3125, 0.1875, 0.125 ]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W=np.array([\n",
    "        [0,0,1/3,0],\n",
    "        [1/2,0,1/3,0],\n",
    "        [1/2,0,0,1],\n",
    "        [0,1,1/3,0]\n",
    "    ])\n",
    "PR(W,prob=1,t=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Now review some NLTK \n",
    "## TOKENIZE\n",
    "## Corpara: body of text\n",
    "## Lexicon:the mean of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ed873a9254d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#unfortunally this fail to tokenize chinere,using jieba instead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize,word_tokenize\n",
    "#unfortunally this fail to tokenize chinere,using jieba instead\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msg=\"黄晓明工作室对此发声明否认，称黄晓明不认识高某，未参与过任何操纵股票的行为；黄晓明未曾受过任何与股票有关的处罚，亦未介入过任何与股票有关的调查。黄晓明工作室还敦促相关方即刻删撤一切虚假信息，以免对公众造成误导并导致己方法律责任的承担。返回搜狐\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jieba' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3bd6e6771b2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcut_all\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'jieba' is not defined"
     ]
    }
   ],
   "source": [
    "list(jieba.cut(msg,cut_all=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.509 seconds.\n",
      "DEBUG:jieba:Loading model cost 1.509 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word 黄晓明,pos:nr\n",
      "word 工作室,pos:n\n",
      "word 对此,pos:d\n",
      "word 发声明,pos:l\n",
      "word 否认,pos:v\n",
      "word ，,pos:x\n",
      "word 称,pos:v\n",
      "word 黄晓明,pos:nr\n",
      "word 不,pos:d\n",
      "word 认识,pos:v\n",
      "word 高某,pos:nr\n",
      "word ，,pos:x\n",
      "word 未,pos:d\n",
      "word 参与,pos:v\n",
      "word 过,pos:ug\n",
      "word 任何,pos:r\n",
      "word 操纵,pos:v\n",
      "word 股票,pos:n\n",
      "word 的,pos:uj\n",
      "word 行为,pos:v\n",
      "word ；,pos:x\n",
      "word 黄晓明,pos:nr\n",
      "word 未曾,pos:d\n",
      "word 受过,pos:v\n",
      "word 任何,pos:r\n",
      "word 与,pos:p\n",
      "word 股票,pos:n\n",
      "word 有关,pos:vn\n",
      "word 的,pos:uj\n",
      "word 处罚,pos:v\n",
      "word ，,pos:x\n",
      "word 亦,pos:d\n",
      "word 未,pos:d\n",
      "word 介入,pos:v\n",
      "word 过,pos:ug\n",
      "word 任何,pos:r\n",
      "word 与,pos:p\n",
      "word 股票,pos:n\n",
      "word 有关,pos:vn\n",
      "word 的,pos:uj\n",
      "word 调查,pos:vn\n",
      "word 。,pos:x\n",
      "word 黄晓明,pos:nr\n",
      "word 工作室,pos:n\n",
      "word 还,pos:d\n",
      "word 敦促,pos:v\n",
      "word 相关,pos:v\n",
      "word 方,pos:n\n",
      "word 即刻,pos:d\n",
      "word 删撤,pos:v\n",
      "word 一切,pos:r\n",
      "word 虚假,pos:a\n",
      "word 信息,pos:n\n",
      "word ，,pos:x\n",
      "word 以免,pos:c\n",
      "word 对,pos:p\n",
      "word 公众,pos:n\n",
      "word 造成,pos:v\n",
      "word 误导,pos:n\n",
      "word 并,pos:c\n",
      "word 导致,pos:v\n",
      "word 己方,pos:n\n",
      "word 法律责任,pos:n\n",
      "word 的,pos:uj\n",
      "word 承担,pos:v\n",
      "word 。,pos:x\n",
      "word 返回,pos:v\n",
      "word 搜狐,pos:nz\n"
     ]
    }
   ],
   "source": [
    "words=[]\n",
    "for char,pos in pseg.cut(msg):\n",
    "#     if pos.startswith('n') or pos.startswith('a'):\n",
    "    print('word %s,pos:%s'%(char,pos))\n",
    "#         words.append(char)\n",
    "words=set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some document about jieba <a href='https://github.com/fxsjy/jieba'>doc</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corpora2Sentence(corpos):\n",
    "    '''\n",
    "        cut corpos into sentences\n",
    "        return:\n",
    "            sentences:list,one element represent a none adj,select[i]=(word,pos) mean word as pos\n",
    "            vocabs:a set of all words occur in corpos\n",
    "            word_idx and idx_word:the map between idx and vocab,I using a matrix representation\n",
    "                for the graph\n",
    "            vocabs_position:a dict,the element look like: \n",
    "                V(i):[p1,p2,..pn],mean word V occur at positon p1,p2,..pn\n",
    "            position_vocabs:reverse of vocabs_position:\n",
    "                pi:V(j),mean at positon pi is word V(j)\n",
    "    '''\n",
    "    sentences=[]\n",
    "    vocabs=[]  #store all vocabulary\n",
    "    vocabs_position={}\n",
    "    word_idx=0 #the word position at corpos\n",
    "    for char,pos in pseg.cut(corpos):\n",
    "        word_idx+=1\n",
    "        if pos=='x':continue\n",
    "        elif (pos.startswith('n') or pos=='a' or pos.startswith('v'))and len(char)>1:\n",
    "            sentences.append((char,word_idx))\n",
    "            vocabs.append(char)\n",
    "            if char in vocabs_position:vlist=vocabs_position[char]\n",
    "            else:\n",
    "                vlist=[]\n",
    "                vocabs_position[char]=vlist\n",
    "            vlist.append(word_idx)\n",
    "        \n",
    "        \n",
    "    vocabs=set(vocabs)\n",
    "    word_idx={c:i for i,c in enumerate(vocabs)}\n",
    "    idx_word={i:c for i,c in enumerate(vocabs)}\n",
    "    \n",
    "    position_vocabs={}\n",
    "    for vocb,poslist in vocabs_position.items():\n",
    "        for p in poslist:position_vocabs[p]=vocb\n",
    "\n",
    "    return sentences,vocabs,word_idx,idx_word,vocabs_position,position_vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I need a little Graph Theory to handle the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#[产品]-[]\n",
    "#[信息化]-[]\n",
    "#[商用车]-[]\n",
    "#[工业]-[]\n",
    "#[新能源]-[汽车,船舶]\n",
    "#[汽车]-[产品,标准]\n",
    "#[船舶]-[标准]\n",
    "#[标准]-[]\n",
    "#[目录]-[]\n",
    "#[车船税]-[]\n",
    "#output should be  信息化 商用车 工业 新能源-汽车-产品 新能源-汽车-标准 新能源-船舶-标准 目录 车船税"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['商用车', '信息化', '车船税', '目录', '工业', '新能源-汽车-产品', '新能源-汽车-标准', '新能源-船舶-标准']\n"
     ]
    }
   ],
   "source": [
    "class Vertex():\n",
    "    '''\n",
    "        I using adjacency list representations,a convinient way to do DFS\n",
    "    \n",
    "    '''\n",
    "    def __init__(self,w):\n",
    "        self.content=w\n",
    "        self.inDegree=0\n",
    "        self.adj=set() #adjacentcy vertexs\n",
    "    def __eq__(self,other):\n",
    "        return self.content==other.content\n",
    "    def __repr__(self):\n",
    "        return self.content\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "    def __hash__(self):\n",
    "        return hash(self.content)\n",
    "class Vsets():\n",
    "    def __init__(self,words):\n",
    "        '''\n",
    "            maintain a set contain all the vertex\n",
    "        '''\n",
    "        self.vertexes={w:Vertex(w) for w in words}\n",
    "    def connect(self,v1,v2):\n",
    "        '''\n",
    "            add a edge from v1->v2\n",
    "            v1,v2 are string(key of self.vertexes)\n",
    "        '''\n",
    "        v1=self.vertexes[v1]\n",
    "        v2=self.vertexes[v2]\n",
    "        v1.adj.add(v2)\n",
    "        v2.inDegree+=1\n",
    "    def DFS(self):\n",
    "        '''\n",
    "            Do Depth First Serch on vertexes,start point is all vertex that\n",
    "            have inDegree of 0,in a other word,it's the root of tree\n",
    "            \n",
    "            return: a list of all the possible combination of content from root to leaf\n",
    "        '''\n",
    "        \n",
    "        #a helper dict member all visited vertex,initial state:no one be visited\n",
    "        self.visited={}\n",
    "        #record all vertex have in degress 0\n",
    "        root=set()\n",
    "        for w,v in self.vertexes.items():\n",
    "            self.visited[v]=False\n",
    "            if v.inDegree==0:\n",
    "                root.add(v)\n",
    "                \n",
    "                \n",
    "        \n",
    "        ret=[]\n",
    "        #do DFS_visit from root\n",
    "        for v in root:\n",
    "            v_list=self.DFS_Visit(v)\n",
    "            ret.extend(v_list)\n",
    "        del self.visited\n",
    "        return ret\n",
    "    def DFS_Visit(self,s):\n",
    "        '''\n",
    "            start from s,DFS to leaf\n",
    "            return: a list of all the possible combination of content from s to it's leaf\n",
    "        '''\n",
    "        self.visited[s]=True #first label this a visited vertex\n",
    "        ret=[]\n",
    "        \n",
    "        \n",
    "        for u in s.adj:\n",
    "            if self.visited[u]==False:\n",
    "                u_list=self.DFS_Visit(u)  #a list of all possible from u,do DFS_VISIT\n",
    "            else:\n",
    "                u_list=self.visited[u] #no need search \n",
    "            #append prefix s.content to subTree\n",
    "            ret.extend([s.content+'-'+c for c in u_list])\n",
    "        \n",
    "        #if s have no subTree to visited\n",
    "        if len(ret)==0:ret.append(s.content)\n",
    "        self.visited[s]=ret\n",
    "        return sorted(ret)\n",
    "#[产品]-[]\n",
    "#[信息化]-[]\n",
    "#[商用车]-[]\n",
    "#[工业]-[]\n",
    "#[新能源]-[汽车,船舶]\n",
    "#[汽车]-[产品,标准]\n",
    "#[船舶]-[标准]\n",
    "#[标准]-[]\n",
    "#[目录]-[]\n",
    "#[车船税]-[]\n",
    "#output should be  信息化 商用车 工业 目录 车船税 新能源-汽车-产品 新能源-汽车-标准 新能源-船舶-标准 \n",
    "\n",
    "vset=Vsets(['产品','信息化','商用车','工业','新能源','汽车','船舶','标准','目录','车船税'])\n",
    "vset.connect('新能源','汽车')\n",
    "vset.connect('新能源','船舶')\n",
    "vset.connect('汽车','产品')\n",
    "vset.connect('汽车','标准')\n",
    "vset.connect('船舶','标准')\n",
    "# vset.connect('新能源','汽车')\n",
    "print(vset.DFS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Sequence Graph Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    def __init__(self,filename):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        #cut corpus to words,remain only none and adjective\n",
    "        with open(filename) as f:\n",
    "            corpora=f.read()\n",
    "            self.sentence,self.vocab,self.word_idx,self.idx_word,self.vocabs_position,self.position_vocabs=corpora2Sentence(corpora)\n",
    "        \n",
    "        self.words=len(self.vocab)\n",
    "        self.H=np.zeros((self.words,self.words)) #matrix representation\n",
    "        \n",
    "    def _connect(self,char1,char2):\n",
    "        '''\n",
    "            char1,char2 is select 2 words by jieba,\n",
    "            char2 is after char2,\n",
    "            if the distance between char1,char2 is less than window_size,\n",
    "            I treat this is a valid edge,then update graph matrix\n",
    "            \n",
    "            char1:tuple(word1,position1)\n",
    "            \n",
    "            return True/False\n",
    "        '''\n",
    "        c1,p1,c2,p2=*char1,*char2\n",
    "        \n",
    "        if(abs(p2-p1)>self.windowSize):return False\n",
    "        idx1,idx2=self.word_idx[c1],self.word_idx[c2]\n",
    "        if idx1!=idx2:self.H[idx1,idx2]=self.H[idx2,idx1]=1\n",
    "        return True\n",
    "    def getMatrix(self,windowSize=5,eps=1e-6):\n",
    "        '''\n",
    "            compute graph matrix,using by Page Rank \n",
    "            windowsSize: within windowsSize ragime of word s view as neibourhood of s\n",
    "        '''\n",
    "        self.windowSize=windowSize\n",
    "        \n",
    "        sentencelen=len(self.sentence)\n",
    "        for i in range(sentencelen):\n",
    "            char1=self.sentence[i]\n",
    "            for j in range(i+1,sentencelen):\n",
    "                char2=self.sentence[j]\n",
    "                if self._connect(char1,char2)==False:break\n",
    "        \n",
    "        degree=np.sum(self.H,axis=0)\n",
    "        degree[degree==0]=eps\n",
    "        H=self.H/degree\n",
    "        return H\n",
    "    def rank(self,indices,top=1000):\n",
    "        '''\n",
    "            indices:the word's index according to Page Rank Algrithm,in decent order\n",
    "            top:select first top word of indices\n",
    "        '''\n",
    "\n",
    "        \n",
    "        selectedWords=[]\n",
    "        for i in range(min(len(indices),top)):\n",
    "            selectedWords.append(self.idx_word[indices[i]])\n",
    "        #using graph tool to handle the output\n",
    "        vstool=Vsets(selectedWords)\n",
    "        \n",
    "        #sort all positon of selected word occur in corpos\n",
    "        sel_pos_list=[]\n",
    "        for w in selectedWords:\n",
    "            sel_pos_list.extend(self.vocabs_position[w])\n",
    "        #sel_pos_list is a list of selectWords's positon ,for example,个税 is a select word,\n",
    "        #occur at position 10,30(reference to corpos),sel_pos_list look like [....10...30.....]\n",
    "        sel_pos_list=sorted(sel_pos_list) \n",
    "        \n",
    "        \n",
    "        '''\n",
    "            no go through sel_pos_list, if sel_pos_list[i] sel_pos_list[i+1] are continue posion ,\n",
    "            that mean W[sel_pos_list[i]] and W[sel_pos_list[i+1]] should merged as one word,\n",
    "            with vstool to add a edge from  W[sel_pos_list[i]] to W[sel_pos_list[i+1]],\n",
    "            vstool can handle consitive word properly!\n",
    "        \n",
    "        '''\n",
    "        for i in range(len(sel_pos_list)-1):\n",
    "            pi=sel_pos_list[i]\n",
    "            Wi=self.position_vocabs[pi]\n",
    "            pj=sel_pos_list[i+1]\n",
    "            Wj=self.position_vocabs[pj]\n",
    "            if(pj-pi==1 and Wi !=Wj):\n",
    "                vstool.connect(Wi,Wj)\n",
    "        ret=vstool.DFS()\n",
    "        return ret,selectedWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph=Graph('PR/tx2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据国务院有关部署，现就调整房地产交易环节契税、营业税优惠政策通知如下：\r\n",
      "\r\n",
      "　　一、关于契税政策\r\n",
      "\r\n",
      "　　（一）对个人购买家庭唯一住房（家庭成员范围包括购房人、配偶以及未成年子女，下同），面积为90平方米及以下的，减按1%的税率征收契税；面积为90平方米以上的，减按1.5%的税率征收契税。\r\n",
      "\r\n",
      "　　（二）对个人购买家庭第二套改善性住房，面积为90平方米及以下的，减按1%的税率征收契税；面积为90平方米以上的，减按2%的税率征收契税。\r\n",
      "\r\n",
      "　　家庭第二套改善性住房是指已拥有一套住房的家庭，购买的家庭第二套住房。\r\n",
      "\r\n",
      "　　（三）纳税人申请享受税收优惠的，根据纳税人的申请或授权，由购房所在地的房地产主管部门出具纳税人家庭住房情况书面查询结果，并将查询结果和相关住房信息及时传递给税务机关。暂不具备查询条件而不能提供家庭住房查询结果的，纳税人应向税务机关提交家庭住房实有套数书面诚信保证，诚信保证不实的，属于虚假纳税申报，按照《中华人民共和国税收征收管理法》的有关规定处理，并将不诚信记录纳入个人征信系统。\r\n",
      "\r\n",
      "　　按照便民、高效原则，房地产主管部门应按规定及时出具纳税人家庭住房情况书面查询结果，税务机关应对纳税人提出的税收优惠申请限时办结。\r\n",
      "\r\n",
      "　　（四）具体操作办法由各省、自治区、直辖市财政、税务、房地产主管部门共同制定。\r\n",
      "\r\n",
      "　　二、关于营业税政策\r\n",
      "\r\n",
      "　　个人将购买不足2年的住房对外销售的，全额征收营业税；个人将购买2年以上（含2年）的住房对外销售的，免征营业税。\r\n",
      "\r\n",
      "　　办理免税的具体程序、购买房屋的时间、开具发票、非购买形式取得住房行为及其他相关税收管理规定，按照《国务院办公厅转发建设部等部门关于做好稳定住房价格工作意见的通知》（国办发〔2005〕26号）、《国家税务总局 财政部 建设部关于加强房地产税收管理的通知》（国税发〔2005〕89号）和《国家税务总局关于房地产税收政策执行中几个具体问题的通知》（国税发〔2005〕172号）的有关规定执行。\r\n",
      "\r\n",
      "　　三、关于实施范围\r\n",
      "\r\n",
      "　　北京市、上海市、广州市、深圳市暂不实施本通知第一条第二项契税优惠政策及第二条营业税优惠政策，上述城市个人住房转让营业税政策仍按照《财政部 国家税务总局关于调整个人住房转让营业税政策的通知》（财税〔2015〕39号）执行。\r\n",
      "\r\n",
      "　　上述城市以外的其他地区适用本通知全部规定。"
     ]
    }
   ],
   "source": [
    "!cat 'PR/tx2.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'减按', '政策', '拥有', '提出', '实施', '中华人民共和国', '应对', '办法', '上海市', '购买', '住房价格', '家庭成员', '执行', '套数', '属于', '提交', '情况', '部门', '发票', '纳入', '深圳市', '纳税', '子女', '个人', '城市', '北京市', '限时', '形式', '家庭', '调整', '建设部', '通知', '程序', '房屋', '保证', '处理', '广州市', '面积', '授权', '诚信', '自治区', '优惠', '征收', '税收', '相关', '具体', '行为', '原则', '具备', '提供', '财政', '财税', '高效', '包括', '国家税务总局', '不能', '书面', '征信', '直辖市', '不实', '契税', '规定', '查询', '纳税人', '税收管理', '办理', '管理法', '稳定', '全部', '范围', '住房', '记录', '交易', '意见', '配偶', '地区', '部署', '制定', '销售', '税务机关', '环节', '转发', '传递', '做好', '申报', '税率', '便民', '改善', '购房人', '税务', '结果', '国务院办公厅', '工作', '有关', '个人住房', '出具', '所在地', '系统', '不足', '购房', '加强', '国务院', '虚假', '取得', '税收政策', '实有', '问题', '适用', '免税', '时间', '主管部门', '下同', '优惠政策', '享受', '财政部', '信息', '申请', '开具', '免征', '全额', '营业税', '办结', '转让', '条件'}\n"
     ]
    }
   ],
   "source": [
    "H=graph.getMatrix(10)\n",
    "print(graph.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03144024 0.02204758 0.02075831 0.01836667 0.01787745 0.01716029\n",
      " 0.0164818  0.01538152 0.01494893 0.01478611]\n",
      "['住房', '家庭', '通知', '购买', '规定', '营业税', '纳税人', '税务机关', '个人', '契税']\n",
      "['税务机关', '营业税', '通知', '个人-购买-家庭-住房', '规定', '纳税人-家庭-住房', '契税']\n"
     ]
    }
   ],
   "source": [
    "rank,rankScore=PR(H)\n",
    "print(rankScore[0:10])\n",
    "a,b=graph.rank(rank,10)\n",
    "print(b)\n",
    "print(a)\n",
    "# print(rankScore)\n",
    "#['产品', '信息化', '商用车', '工业', '新能源', '标准', '汽车', '目录', '船舶', '车船税']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
